{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLX Predictive Maintenance\n",
    "\n",
    "This is an introduction to CLX Predictive Maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Like any other Linux based machine, DGX's generate a vast amount of logs. Analysts spend hours trying to identify the root causes of each failure. There could be infinitely many types of root causes of the failures. Some patterns might help to narrow it down; however, regular expressions can only help to identify previously known patterns. Moreover, this creates another manual task of maintaining a search script.\n",
    "\n",
    "CLX predicitive maintenance module shows us how GPU's can accelerate the analysis of the enormous amount of logs using machine learning. Another benefit of analyzing in a probabilistic way is that we can pin down unseen root causes. To achieve this, we will fine-tune a pre-trained BERT* model with a classification layer using HuggingFace library.\n",
    "\n",
    "Once the model is capable of identifying even the new root causes, it can also be deployed as a process running in the machines to predict failures before they happen.\n",
    "\n",
    "*BERT stands for Bidirectional Encoder Representations from Transformers. The paper can be found [here.](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train a Predictive Maintenance model\n",
    "\n",
    "To train a CLX Predictive Maintenance model you simply need a training dataset which contains a column of `log` and their associated `label` which can be either `0` for `ordinary` or `1` for `root cause`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initialize your new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from clx.analytics.sequence_classifier import SequenceClassifier;\n",
    "\n",
    "seq_classifier = SequenceClassifier()\n",
    "seq_classifier.init_model(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train your predictive maintenance model. The below example uses a small sample dataset for demonstration only. Ideally you will want a larger training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "df = cudf.DataFrame()\n",
    "df[\"log\"] = [\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: [ 1021.384311] docker0: port 1(veth3dd105f) entered blocking state\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: [ 1021.384315] docker0: port 1(veth3dd105f) entered disabled state\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: [ 1021.384418] device veth3dd105f entered promiscuous mode\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: docker0: port 1(veth3dd105f) entered blocking state\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: docker0: port 1(veth3dd105f) entered disabled state\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: device veth3dd105f entered promiscuous mode\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: [ 1021.654834] eth0: renamed from veth7677340\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: eth0: renamed from veth7677340\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: [ 1021.686871] IPv6: ADDRCONF(NETDEV_CHANGE): veth3dd105f: link becomes ready\",\n",
    "    \"Apr  6 15:17:57 local-dt-eno1 kernel: [ 1021.686944] docker0: port 1(veth3dd105f) entered blocking state\",\n",
    "]\n",
    "df[\"label\"] = [1, 0, 1, 0, 1, 0, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, 'label', train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8160929083824158\n",
      "Validation Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seq_classifier.train_model(X_train[\"log\"], y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_classifier.evaluate_model(X_test[\"log\"], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, you will want to train your model over a number of `epochs` as detailed in our example Predictive Maintenance [notebook](https://github.com/rapidsai/clx/blob/branch-0.19/notebooks/Predictive_Maintenance/Predictive_Maintenance_Sequence_Classifier.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a trained model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_classifier.save_model(\"clx_pdm_classifier.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_classifier.save_model(\"clx_pdm_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model\n",
    "\n",
    "Let's create a new sequence classifier instance and load saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdm = SequenceClassifier()\n",
    "pdm.init_model(\"clx_pdm_classifier.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new sequence classifier instance and load saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdm2 = SequenceClassifier()\n",
    "pdm2.init_model(\"clx_pdm_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDM Inferencing\n",
    "\n",
    "Use your new model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    1\n",
       " dtype: uint8,\n",
       " 0    0.516594\n",
       " dtype: float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_df = cudf.DataFrame()\n",
    "infer_df[\"log\"] = [\"Apr  6 15:07:07 local-dt-eno1 kernel: [  371.072371] audit: type=1400 audit(1617721627.183:67): apparmor=\\\"STATUS\\\" operation=\\\"profile_load\\\" profile=\\\"unconfined\\\" name=\\\"snap-update-ns.cmake\\\" pid=7066 comm=\\\"apparmor_parser\\\"\"]\n",
    "\n",
    "pdm.predict(infer_df[\"log\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This example shows how to use a BERT-based predictive maintenance. This approach can be implemented on the machines to warn the users well before the problems occur so corrective actions can be taken."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
