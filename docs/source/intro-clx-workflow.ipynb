{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLX Workflow\n",
    "\n",
    "This is an introduction to the CLX Workflow and it's I/O components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a CLX Workflow?\n",
    "\n",
    "A CLX Workflow receives data from a particular source, performs operations on that data within a GPU dataframe, and outputs that data to a particular destination. This guide will teach you how to configure your workflow inputs and outputs around a simple workflow example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use a CLX Workflow\n",
    "\n",
    "A CLX Workflow provides a simple and modular way of \"plugging in\" a particular workflow to a read from different inputs and outputs. Use a CLX Workflow when you would like to deploy a workflow as part of a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple example of a custom Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clx.workflow.workflow import Workflow\n",
    "class CustomWorkflow(Workflow):\n",
    "    def workflow(self, dataframe):\n",
    "        dataframe[\"enriched\"] = \"enriched output\"\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Workflow relies on the Workflow class which handles the I/O and general data processing functionality. To implement a new Workflow, the developer need only implement the `workflow` function which receives an input dataframe, as shown above. \n",
    "  \n",
    "A more advanced example of a Worlflow can be found [here](https://github.com/rapidsai/clx/blob/branch-0.12/clx/workflow/splunk_alert_workflow.py).\n",
    "It is an example of a [Splunk](https://www.splunk.com/) Alert Workflow used to find anamolies in Splunk alert data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow I/O Components\n",
    "\n",
    "In order to deploy a workflow to an input an output data feed, we integrate the CLX I/O components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look quickly at what a workflow configuration for the source and destination might look like. You can see below we declare each of the properties within a dictionary. For more information on how to declare configuration within a configurable yaml file [go]. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = {\n",
    "   \"type\": \"fs\",\n",
    "   \"input_format\": \"csv\",\n",
    "   \"input_path\": \"/full/path/to/input/data\",\n",
    "   \"schema\": [\"raw\"],\n",
    "   \"delimiter\": \",\",\n",
    "   \"required_cols\": [\"raw\"],\n",
    "   \"dtype\": [\"str\"],\n",
    "   \"header\": 0\n",
    "}\n",
    "destination = {\n",
    "   \"type\": \"fs\",\n",
    "   \"output_format\": \"csv\",\n",
    "   \"output_path\": \"/full/path/to/output/data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first step to configuring the input and output of a workflow is to determine the source and destination type. Then to set the associated parameters for that specific type.\n",
    "As seen above the `type` property is listed first and can be one of the following.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Source Types\n",
    "  \n",
    "* `fs` - Read from a local filesystem\n",
    "* `dask_fs` - Increase the speed of GPU workflow operations by reading from a file using Dask\n",
    "* `kafka` - Read from [Kafka](https://kafka.apache.org/)\n",
    "\n",
    "Destination Types\n",
    "  \n",
    "* `fs` - Writing to local filesystem\n",
    "* `kafka` - Write to [Kafka](https://kafka.apache.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Source and Destination Configurations\n",
    "\n",
    "#### Filesystem\n",
    "If the `fs` type is used, the developer must distinguish the data format using the `input_format` attribute. Formats available are: csv, parquet, and orc.\n",
    "  \n",
    "The associated parameters available for the `fs` type and `input_format` are documented within the [cuDF I/O](https://docs.rapids.ai/api/cudf/0.11/api.html#module-cudf.io.csv) API. For example for reading data from a csv file, reference [cudf.io.csv.read_csv](https://docs.rapids.ai/api/cudf/0.11/api.html#cudf.io.csv.read_csv) available parameters.\n",
    "\n",
    "Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = {\n",
    "   \"type\": \"fs\",\n",
    "   \"input_format\": \"parquet\",\n",
    "   \"input_path\": \"/full/path/to/input/data\",\n",
    "   \"columns\": [\"x\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask Filesystem\n",
    "  \n",
    "If the `dask_fs` type is used the developer must distinguish the data format using the `input_format` attribute. Formats available are: csv, parquet, and orc.\n",
    "  \n",
    "The associated parameters available for the `dask_fs` type and `input_format` are listed within the [Dask cuDF](https://docs.rapids.ai/api/cudf/0.11/10min.html#Getting-Data-In/Out) documentation. \n",
    "  \n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = {\n",
    "   \"type\": \"dask_fs\",\n",
    "   \"input_format\": \"csv\",\n",
    "   \"input_path\": \"/full/path/to/input/data/*.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kafka\n",
    "If the `kafka` type is used the following parameters must be indicated  \n",
    "  \n",
    "Source  \n",
    "  \n",
    "* `kafka_brokers` - Kafka brokers\n",
    "* `group_id` - Group ID for consuming kafka messages\n",
    "* `consumer_kafka_topics` - Names of kafka topics to read from\n",
    "* `batch_size` - Indicates number of kafka messages to read before data is processed through the workflow\n",
    "* `time_window` - Maximum time window to wait for `batch_size` to be reached before workflow processing begins. \n",
    "  \n",
    "Destination  \n",
    "  \n",
    "* `kafka_brokers` - Kafka brokers\n",
    "* `publisher_kafka_topic` - Names of kafka topic to write data to\n",
    "* `batch_size` - Indicates number of workflow-processed messages to aggregate before data is written to the kafka topic\n",
    "* `output_delimiter` - Delimiter of the data columns\n",
    "  \n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"kafka_brokers\": \"kafka:9092\",\n",
    "    \"group_id\": \"cyber\",\n",
    "    \"batch_size\": 10,\n",
    "    \"consumer_kafka_topics\": [\"topic1\", \"topic2\"],\n",
    "    \"time_window\": 5\n",
    "}\n",
    "dest = {\n",
    "    \"type\": \"kafka\",\n",
    "    \"kafka_brokers\": \"kafka:9092\",\n",
    "    \"batch_size\": 10,\n",
    "    \"publisher_kafka_topic\": \"topic3\",\n",
    "    \"output_delimiter\": \",\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying it together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have established our workflow and source and destination configurations we can now run our workflow. Let's create a workflow using the `CustomWorkflow` we created above.\n",
    "\n",
    "Firstly, we must know the parameters for instantiating a basic workflow\n",
    "  \n",
    "* `name` - The name of the workflow\n",
    "* `source` - The source of input data (optional)\n",
    "* `destination` - The destination for output data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clx.workflow.workflow import Workflow\n",
    "class CustomWorkflow(Workflow):\n",
    "    def workflow(self, dataframe):\n",
    "        dataframe[\"enriched\"] = \"enriched output\"\n",
    "        return dataframe\n",
    "    \n",
    "source = {\n",
    "   \"type\": \"fs\",\n",
    "   \"input_format\": \"csv\",\n",
    "   \"input_path\": \"/full/path/to/input/data\",\n",
    "   \"schema\": [\"raw\"],\n",
    "   \"delimiter\": \",\",\n",
    "   \"required_cols\": [\"raw\"],\n",
    "   \"dtype\": [\"str\"],\n",
    "   \"header\": 0\n",
    "}\n",
    "destination = {\n",
    "   \"type\": \"fs\",\n",
    "   \"output_format\": \"csv\",\n",
    "   \"output_path\": \"/full/path/to/output/data\"\n",
    "}\n",
    "\n",
    "my_new_workflow = CustomWorkflow(source=source, destination=destination, name=\"my_new_workflow\")\n",
    "my_new_workflow.run_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow configurations in an external file\n",
    "\n",
    "Sometimes workflow configurations may need to change dependent upon the environment. To avoid declaring workflow configurations within sourcecode you may also declare them in an external yaml file. A workflow will look for and establish I/O connections by searching for configurations in the following order:\n",
    "  \n",
    "1. /etc/clx/[workflow-name]/workflow.yaml\n",
    "1. ~/.config/clx/[workflow-name]/workflow.yaml\n",
    "1. In-line python config\n",
    "  \n",
    "If source and destination are indicated in external files, they are not required to instantiate a new workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow config located at /etc/clx/my_new_workflow/workflow.yaml\n",
    "my_new_workflow = CustomWorkflow(name=\"my_new_workflow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
