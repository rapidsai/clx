{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a guide on how [CLX](https://github.com/rapidsai/clx) and [Streamz](https://streamz.readthedocs.io/en/latest/) can be used to build a streaming inference pipeline.\n",
    "\n",
    "Streamz has the ability to read from [Kafka](https://kafka.apache.org/) directly into [Dask](https://dask.org/) allowing for computation on a multi-core or cluster environment. This approach is best used for instances in which you hope to increase processing speeds with streaming data.\n",
    "\n",
    "A selection of workflows such as cyBERT and DGA detection inferencing are implemented in CLX streamz. Here we share an example in which we demonstrate how to read Apache log data from Kafka, perform log parsing using CLX cyBERT and publish result data back to Kafka. Similarly, also for DGA detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Quickstart Docker Image\n",
    "\n",
    "For convenience, you can build a Docker image that will include a working environment that's ready for running your pipeline. This image will contain all needed components including [Kafka](https://kafka.apache.org/) and [Zookeeper](https://zookeeper.apache.org/).\n",
    "\n",
    "Prerequisites:\n",
    "* NVIDIA Pascalâ„¢ GPU architecture or better\n",
    "* CUDA 10.1+ compatible NVIDIA driver\n",
    "* Ubuntu 16.04/18.04 or CentOS 7\n",
    "* Docker CE v18+\n",
    "* nvidia-docker v2+\n",
    "\n",
    "Run the following to build the image:\n",
    "\n",
    "`\n",
    "docker build -f examples/streamz/Dockerfile -t clx-streamz:latest .\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Docker Container\n",
    "\n",
    "A Docker container is created using the image above. The 'docker run' format to build your container is shown below. Note: volume binding to the container is an optional argument.\n",
    "\n",
    "**Preferred - Docker CE v19+ and nvidia-container-toolkit**\n",
    "\n",
    "```\n",
    "docker run -it \\\n",
    "    -p 9787:8787 \\\n",
    "    -p 9888:8888 \\\n",
    "    -v <your_volume_binding_host_directory_path>:<your_volume_binding_container_directory_path> \\\n",
    "    --gpus '\"device=0,1,2\"' \\\n",
    "    --name clx_streamz \\\n",
    "    -d clx-streamz:latest\n",
    "```\n",
    "\n",
    "**Legacy - Docker CE v18 and nvidia-docker2**\n",
    "\n",
    "```\n",
    "docker run -it \\\n",
    "    -p 9787:8787 \\\n",
    "    -p 9888:8888 \\\n",
    "     -v <your_volume_binding_host_directory_path>:<your_volume_binding_container_directory_path> \\\n",
    "    --runtime=nvidia \\\n",
    "    --name clx_streamz \\\n",
    "    -d cybert-streamz:latest\n",
    "```\n",
    "\n",
    "The Dockerfile contains an ENTRYPOINT which calls [entrypoint.sh](https://github.com/rapidsai/clx/blob/branch-0.17/examples/streamz/scripts/entrypoint.sh) to:\n",
    "1. Configure and install Kafka\n",
    "2. Run Kafka broker on `localhost:9092` and Zookeeper on `localhost:2181`\n",
    "3. Creates (cyBERT and DGA detection) specific input and output kafka topics and publishes sample input data \n",
    "\n",
    "Your Quickstart Docker container includes the data and models required to run cyBERT and DGA detection stream processing workflows.\n",
    "\n",
    "## Run cyBERT Streamz Example on Apache Logs\n",
    "```\n",
    "docker exec clx_streamz bash -c 'source activate rapids \\\n",
    "    && python $CLX_STREAMZ_HOME/python/cybert.py \\\n",
    "    --broker localhost:9092 \\\n",
    "    --input_topic cybert_input \\\n",
    "    --output_topic cybert_output \\\n",
    "    --group_id streamz \\\n",
    "    --model $CLX_STREAMZ_HOME/ml/models/cybert/pytorch_model.bin \\\n",
    "    --label_map $CLX_STREAMZ_HOME/ml/models/cybert/config.json \\\n",
    "    --poll_interval 1s \\\n",
    "    --max_batch_size 500'\n",
    "```\n",
    "\n",
    "## Run DGA Streamz Example on Sample Domains\n",
    "```\n",
    "docker exec clx_streamz bash -c 'source activate rapids \\\n",
    "    && python $CLX_STREAMZ_HOME/python/dga_detection.py \\\n",
    "    --broker localhost:9092 \\\n",
    "    --input_topic dga_detection_input \\\n",
    "    --output_topic dga_detection_output \\\n",
    "    --group_id streamz \\\n",
    "    --model $CLX_STREAMZ_HOME/ml/models/dga/pytorch_model.bin \\\n",
    "    --poll_interval 1s \\\n",
    "    --max_batch_size 500'\n",
    "```\n",
    "\n",
    "Processed data will be pushed to the given kafka output topic. To view all processed output run:\n",
    "\n",
    "```\n",
    "docker exec clx_streamz bash -c 'source activate rapids \\\n",
    "       && $KAFKA_HOME/bin/kafka-console-consumer.sh \\\n",
    "       --bootstrap-server <broker> \\\n",
    "       --topic <output_topic> \\\n",
    "       --from-beginning'\n",
    "```\n",
    "\n",
    "View the data processing activity on the dask dashboard by visiting http://localhost:9787 or `<host>:9787`\n",
    "\n",
    "## Capturing Benchmarks\n",
    "To capture benchmarks add the benchmark flag along with average log size (kb), for throughput (mb/s) and average batch size (mb) estimates, to the docker run command above. In this case, we are benchmarking the cyBERT workflow with the commands below. Similarly, we can also do it for the DGA detection workflow.\n",
    "```\n",
    "docker exec clx_streamz bash -c 'source activate rapids \\\n",
    "    && python $CLX_STREAMZ_HOME/python/cybert.py \\\n",
    "    --broker localhost:9092 \\\n",
    "    --input_topic cybert_input \\\n",
    "    --output_topic cybert_output \\\n",
    "    --group_id streamz \\\n",
    "    --model $CLX_STREAMZ_HOME/ml/models/cybert/pytorch_model.bin \\\n",
    "    --label_map $CLX_STREAMZ_HOME/ml/models/cybert/config.json \\\n",
    "    --poll_interval 1s \\\n",
    "    --max_batch_size 500 \\\n",
    "    --benchmark 20' \\\n",
    "    > cybert_workflow.log 2>&1 &\n",
    "```\n",
    "\n",
    "To print benchmark, send a SIGINT signal to the running cybert process.\n",
    "```\n",
    "# To get the PID\n",
    "$ docker exec clx_streamz ps aux | grep \"cybert\\.py\" | awk '{print $2}'\n",
    "# Kill process\n",
    "$ docker exec clx_streamz kill -SIGINT <pid>\n",
    "$ less cybert_workflow.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Run Workflow with Custom Arguments\n",
    "\n",
    "1. Create kafka topics for the clx_streamz workflows that you want to run and publish input data.\n",
    "\n",
    "    ```\n",
    "    docker exec clx_streamz /bin/bash -c 'source activate rapids \\\n",
    "        && $CLX_STREAMZ_HOME/scripts/kafka_topic_setup.sh \\\n",
    "        -b localhost:9092 \\\n",
    "        -i <input_topic> \\\n",
    "        -o <output_topic> \\\n",
    "        -d <data_filepath>'\n",
    "    ```\n",
    "    \n",
    "2. Start workflow \n",
    "    \n",
    "    ```\n",
    "    docker exec clx_streamz bash -c 'source activate rapids \\\n",
    "        && python $CLX_STREAMZ_HOME/python/<workflow_script> \\\n",
    "        --broker <host:port> \\\n",
    "        --input_topic <input_topic> \\\n",
    "        --output_topic <output_topic> \\\n",
    "        --group_id <kafka_consumer_group_id> \\\n",
    "        --model <model filepath> \\\n",
    "        --label_map <labels filepath> \\\n",
    "        --poll_interval <poll_interval> \\\n",
    "        --max_batch_size <max_batch_size> \\\n",
    "        --benchmark <avg log size>'\n",
    "    ```\n",
    "    **Parameters:**\n",
    "    - `broker`* - Host and port where kafka broker is running. \n",
    "    - `group_id`* - Kafka [group id](https://docs.confluent.io/current/installation/configuration/consumer-configs.html#group.id) that uniquely identifies the streamz data consumer.\n",
    "    - `input_topic` - The name for the input topic to consumer data.\n",
    "    - `output_topic` - The name for the output topic to send the output data.\n",
    "    - `model_file` - The path to your model file\n",
    "    - `label_file` - The path to your label file\n",
    "    - `poll_interval`* - Interval (in seconds) to poll the Kafka input topic for data (Ex: 60s)\n",
    "    - `max_batch_size`* - Max batch size of data (max number of logs) to ingest into streamz with each `poll_interval` \n",
    "    - `benchmark` - To capture benchmarks add the benchmark flag along with average log size (kb), for throughput (mb/s) and average batch size (mb) estimates.\n",
    "\n",
    "    ``*`` = More information on these parameters can be found in the streamz [documentation](https://streamz.readthedocs.io/en/latest/api.html#streamz.from_kafka_batched)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}