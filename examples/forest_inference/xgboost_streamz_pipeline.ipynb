{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost and Streamz\n",
    "\n",
    "This notebook is a CPU comparison of the [Forest Inference Library](https://medium.com/rapids-ai/rapids-forest-inference-library-prediction-at-100-million-rows-per-second-19558890bc35) (FIL) and [cuStreamz](https://medium.com/rapids-ai/gpu-accelerated-stream-processing-with-rapids-f2b725696a61) pipeline using RAPIDS on the GPU.\n",
    "\n",
    "View the GPU version of the [FIL custreamz pipeline](./FIL_custreamz_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "from os import path\n",
    "\n",
    "# Download sample data and model\n",
    "IOT_MALWARE_JSON=\"iot_malware_1_1.json\"\n",
    "IOT_XGBOOST_MODEL=\"iot_xgboost_model.bst\"\n",
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost model\n",
    "if not path.exists(IOT_XGBOOST_MODEL):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + IOT_XGBOOST_MODEL, IOT_XGBOOST_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoT data in json format\n",
    "if not path.exists(IOT_MALWARE_JSON):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + IOT_MALWARE_JSON, IOT_MALWARE_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our kafka broker already running at `localhost:9092` and input kafka topic created, next we ingest our sample data into our topic named `input`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the data into kafka use the command line tool kafka-console-producer provided by your kafka installation. In this example kafka is installed at /opt/kafka.\n",
    "# Update the broker-list and topic parameters as needed\n",
    "!/opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic input < $IOT_MALWARE_JSON >/dev/null\n",
    "\n",
    "# Optionally repeat this process to populate the kafka queue with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import confluent_kafka as ck\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from streamz import Stream\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average log size is used later in the notebook to estimate throughput and avg batch size benchmarks for streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "avg_log_size=0.478 # in kilobytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the filepath of your FIL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIL\n",
    "model_file=IOT_XGBOOST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka\n",
    "broker=\"localhost:9092\"\n",
    "input_topic=\"input\"\n",
    "output_topic=\"output\"\n",
    "\n",
    "producer_conf = {\n",
    "    \"bootstrap.servers\": broker,\n",
    "    \"session.timeout.ms\": 10000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create your dask cuda cluster and initialize each dask worker with the FIL model referenced above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init():\n",
    "    # Initialization for each dask worker\n",
    "    import xgboost as xgb\n",
    "    worker = dask.distributed.get_worker()\n",
    "    worker.data[\"cpu_model\"] = xgb.Booster()\n",
    "    worker.data[\"cpu_model\"].load_model(model_file)\n",
    "    worker.data[\"cpu_model\"].set_param({\"predictor\": \"cpu_predictor\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:33167': None,\n",
       " 'tcp://127.0.0.1:33365': None,\n",
       " 'tcp://127.0.0.1:34235': None,\n",
       " 'tcp://127.0.0.1:35075': None,\n",
       " 'tcp://127.0.0.1:36583': None,\n",
       " 'tcp://127.0.0.1:36705': None,\n",
       " 'tcp://127.0.0.1:36853': None,\n",
       " 'tcp://127.0.0.1:39381': None,\n",
       " 'tcp://127.0.0.1:44477': None,\n",
       " 'tcp://127.0.0.1:46621': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.run(worker_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:42097' processes=10 threads=80, memory=540.94 GB>\n"
     ]
    }
   ],
   "source": [
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamz Pipeline\n",
    "\n",
    "Update the `max_batch_size` and `poll_interval` parameters as needed to tune your streamz workload to suit your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size=900000\n",
    "poll_interval=\"1s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique group_id to be able to re-run this demo notebook on the same data loaded to your kafka topic.\n",
    "j = random.randint(0,10000)\n",
    "group_id=\"fil-group-%d\" % j\n",
    "\n",
    "# Kafka consumer configuration\n",
    "consumer_conf = {\n",
    "    \"bootstrap.servers\": broker,\n",
    "    \"group.id\": group_id,\n",
    "    \"session.timeout.ms\": \"60000\",\n",
    "    \"enable.partition.eof\": \"true\",\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "}\n",
    "\n",
    "source = Stream.from_kafka_batched(\n",
    "        input_topic,\n",
    "        consumer_conf,\n",
    "        poll_interval=poll_interval,\n",
    "        npartitions=1,\n",
    "        asynchronous=True,\n",
    "        dask=True,\n",
    "        max_batch_size=max_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the `predict` function to be used in the streamz pipeline. The predict function will construct a GPU dataframe of the raw log messages from kafka, format the data and then execute a prediction using the FIL model we previously loaded into Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(messages):\n",
    "    batch_start_time = int(round(time.time()))\n",
    "    worker = dask.distributed.get_worker()\n",
    "    df = pd.DataFrame()\n",
    "    if type(messages) == str:\n",
    "       df[\"stream\"] = [messages.decode('utf-8')]\n",
    "    elif type(messages) == list and len(messages) > 0:\n",
    "       df[\"stream\"] = [msg.decode('utf-8') for msg in messages]\n",
    "    else:\n",
    "       print(\"ERROR: Unknown type encountered in inference\")\n",
    "    df['stream'] = df['stream'].astype(str)\n",
    "    df_conn = pd.json_normalize(df['stream'].apply(json.loads))\n",
    "    cpu_preds = pd.DataFrame()\n",
    "    Dmatrix = xgb.DMatrix(df_conn[[\"resp_ip_bytes\", \"resp_pkts\", \"orig_ip_bytes\", \"orig_pkts\"]])\n",
    "    cpu_preds['predictions'] = worker.data[\"cpu_model\"].predict(Dmatrix)\n",
    "    size = len(cpu_preds)\n",
    "    return (cpu_preds, batch_start_time, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sink_to_kafka` function writes the output data or FIL predictions to the previously defined kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sink_to_kafka(processed_data):\n",
    "    producer = ck.Producer(producer_conf)\n",
    "    json_str = processed_data[0].to_json(orient=\"records\", lines=True)\n",
    "    json_recs = json_str.split(\"\\n\")\n",
    "    print(json_recs)\n",
    "    for idx,rec in enumerate(json_recs):\n",
    "        if idx % 50000 == 0:\n",
    "            producer.flush()\n",
    "        producer.produce(output_topic, rec)\n",
    "    producer.flush()\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define our streamz pipeline. This pipeline is also designed to capture benchmark data for reading and processing FIL predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = source.map(predict).map(lambda x: (x[0], x[1], int(round(time.time())), x[2])).map(sink_to_kafka).gather().sink_to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start the streamz pipeline. View the progress on your dask dashboard http://localhost:8787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the benchmark. With each batch of data processed we have recorded the start and stop times that we can then use to calculate the total time difference. Throughput and avg batch size are estimates based on the average log size previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_benchmark(results, size_per_log):\n",
    "    t1 = int(round(time.time() * 1000))\n",
    "    t2 = 0\n",
    "    size = 0.0\n",
    "    batch_count = 0\n",
    "    cnt = 0\n",
    "    # Find min and max time while keeping track of batch count and size\n",
    "    for result in results:\n",
    "        (ts1, ts2, result_size) = (result[1], result[2], result[3])\n",
    "        cnt += result_size\n",
    "        if ts1 == 0 or ts2 == 0:\n",
    "            continue\n",
    "        batch_count = batch_count + 1\n",
    "        t1 = min(t1, ts1)\n",
    "        t2 = max(t2, ts2)\n",
    "        size += result_size * size_per_log\n",
    "    time_diff = t2 - t1\n",
    "    throughput_mbps = size / (1024.0 * time_diff) if time_diff > 0 else 0\n",
    "    avg_batch_size = size / (1024.0 * batch_count) if batch_count > 0 else 0\n",
    "    return (time_diff, throughput_mbps, avg_batch_size, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please wait a few moments for all logs to be processed before calculating benchmarks  \n",
    "View the progress on the dask dashboard http://localhost:8787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max batch size: 900000\n",
      "poll interval: 1s\n",
      "time (s): 653\n",
      "throughput (mb/s): 35.74253378038859\n",
      "avg batch size (mb): 416.7834742606027\n",
      "num records: 50000066\n"
     ]
    }
   ],
   "source": [
    "benchmark = calc_benchmark(output, avg_log_size)\n",
    "print(\"max batch size:\", max_batch_size)\n",
    "print(\"poll interval:\", poll_interval)\n",
    "print(\"time (s):\", benchmark[0])\n",
    "print(\"throughput (mb/s):\", benchmark[1])\n",
    "print(\"avg batch size (mb):\", benchmark[2])\n",
    "print(\"num records:\", benchmark[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the GPU version view this notebook - [FIL Streamz GPU](./FIL_streamz_pipeline_GPU.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
