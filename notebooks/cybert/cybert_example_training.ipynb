{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cyBERT: a flexible log parser based on the BERT language model\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Generating Labeled Logs\n",
    "* Subword Tokenization\n",
    "* Data Loading\n",
    "* Fine-tuning pretrained BERT\n",
    "* Model Evaluation\n",
    "* Parsing with cyBERT\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One of the most arduous tasks of any security operation (and equally as time consuming for a data scientist) is ETL and parsing. This notebook illustrates how to train a BERT language model using a toy dataset of just 1000 previously parsed apache server logs as a labeled data. We will fine-tune a pretrained BERT model from [HuggingFace](https://github.com/huggingface) with a classification layer for Named Entity Recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "import torch.nn.functional as F\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "from os import path\n",
    "\n",
    "import cupy\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Labels For Our Training Dataset\n",
    "\n",
    "To train our model we begin with a dataframe containing parsed logs and additional `raw` column containing the whole raw log as a string. We will use the column names as our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download log data\n",
    "APACHE_SAMPLE_CSV = \"apache_sample_1k.csv\"\n",
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\"\n",
    "\n",
    "if not path.exists(APACHE_SAMPLE_CSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + APACHE_SAMPLE_CSV, APACHE_SAMPLE_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = cudf.read_csv(APACHE_SAMPLE_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_level</th>\n",
       "      <th>error_message</th>\n",
       "      <th>raw</th>\n",
       "      <th>remote_host</th>\n",
       "      <th>remote_logname</th>\n",
       "      <th>remote_user</th>\n",
       "      <th>request_header_referer</th>\n",
       "      <th>request_header_user_agent</th>\n",
       "      <th>request_header_user_agent__browser__family</th>\n",
       "      <th>request_header_user_agent__browser__version_string</th>\n",
       "      <th>...</th>\n",
       "      <th>request_url_username</th>\n",
       "      <th>response_bytes_clf</th>\n",
       "      <th>status</th>\n",
       "      <th>time_received</th>\n",
       "      <th>time_received_datetimeobj</th>\n",
       "      <th>time_received_isoformat</th>\n",
       "      <th>time_received_tz_datetimeobj</th>\n",
       "      <th>time_received_tz_isoformat</th>\n",
       "      <th>time_received_utc_datetimeobj</th>\n",
       "      <th>time_received_utc_isoformat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>100.1.14.108 - - [30/Sep/2019:22:52:31 +0200] ...</td>\n",
       "      <td>100.1.14.108</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Python/3.7 aiohttp/3.6.1</td>\n",
       "      <td>Python/3.7 aiohttp</td>\n",
       "      <td>3.6.1</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>219</td>\n",
       "      <td>404.0</td>\n",
       "      <td>[30/Sep/2019:22:52:31 +0200]</td>\n",
       "      <td>1.569884e+12</td>\n",
       "      <td>2019-09-30T22:52:31</td>\n",
       "      <td>1.569877e+12</td>\n",
       "      <td>2019-09-30T22:52:31+02:00</td>\n",
       "      <td>1.569877e+12</td>\n",
       "      <td>2019-09-30T20:52:31+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    error_level error_message  \\\n",
       "583        <NA>          <NA>   \n",
       "\n",
       "                                                   raw   remote_host  \\\n",
       "583  100.1.14.108 - - [30/Sep/2019:22:52:31 +0200] ...  100.1.14.108   \n",
       "\n",
       "    remote_logname remote_user request_header_referer  \\\n",
       "583              -           -                      -   \n",
       "\n",
       "    request_header_user_agent request_header_user_agent__browser__family  \\\n",
       "583  Python/3.7 aiohttp/3.6.1                         Python/3.7 aiohttp   \n",
       "\n",
       "    request_header_user_agent__browser__version_string  ...  \\\n",
       "583                                              3.6.1  ...   \n",
       "\n",
       "     request_url_username response_bytes_clf status  \\\n",
       "583                  <NA>                219  404.0   \n",
       "\n",
       "                    time_received time_received_datetimeobj  \\\n",
       "583  [30/Sep/2019:22:52:31 +0200]              1.569884e+12   \n",
       "\n",
       "     time_received_isoformat time_received_tz_datetimeobj  \\\n",
       "583      2019-09-30T22:52:31                 1.569877e+12   \n",
       "\n",
       "     time_received_tz_isoformat time_received_utc_datetimeobj  \\\n",
       "583   2019-09-30T22:52:31+02:00                  1.569877e+12   \n",
       "\n",
       "     time_received_utc_isoformat  \n",
       "583    2019-09-30T20:52:31+00:00  \n",
       "\n",
       "[1 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample parsed log\n",
    "logs_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.108.213.19 - - [18/Jul/2018:21:53:07 +0200] \"GET / HTTP/1.1\" 200 10439 \"-\" \"Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)\" \"-\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample raw log\n",
    "print(logs_df.raw.loc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(index_no, cols):\n",
    "    \"\"\"\n",
    "    label the words in the raw log with the column name from the parsed log\n",
    "    \"\"\"\n",
    "    raw_split = logs_df.raw_preprocess[index_no].split()\n",
    "    \n",
    "    # words in raw but not in parsed logs labeled as 'other'\n",
    "    label_list = ['other'] * len(raw_split) \n",
    "    \n",
    "    # for each parsed column find the location of the sequence of words (sublist) in the raw log\n",
    "    for col in cols:\n",
    "        if str(logs_df[col][index_no]) not in {'','-','None','NaN'}:\n",
    "            sublist = str(logs_df[col][index_no]).split()\n",
    "            sublist_len=len(sublist)\n",
    "            match_count = 0\n",
    "            for ind in (i for i,el in enumerate(raw_split) if el==sublist[0]):\n",
    "                # words in raw log not present in the parsed log will be labeled with 'other'\n",
    "                if (match_count < 1) and (raw_split[ind:ind+sublist_len]==sublist) and (label_list[ind:ind+sublist_len] == ['other'] * sublist_len):\n",
    "                    label_list[ind:ind+sublist_len] = [col] * sublist_len\n",
    "                    match_count = 1\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df['raw_preprocess'] = logs_df.raw.str.replace('\"','')\n",
    "\n",
    "# column names to use as lables\n",
    "cols = logs_df.columns.values.tolist()\n",
    "\n",
    "# do not use raw columns as labels\n",
    "cols.remove('raw')\n",
    "cols.remove('raw_preprocess')\n",
    "\n",
    "# using for loop for labeling funcition until string UDF capability in rapids- it is currently slow\n",
    "labels = []\n",
    "for indx in range(len(logs_df)):\n",
    "    labels.append(labeler(indx, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['remote_host', 'other', 'other', 'time_received', 'time_received', 'request_method', 'request_url', 'other', 'other', 'response_bytes_clf', 'other', 'request_header_user_agent', 'request_header_user_agent', 'request_header_user_agent', 'request_header_user_agent', 'other']\n"
     ]
    }
   ],
   "source": [
    "print(labels[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword Labeling\n",
    "We are using the `bert-base-cased` tokenizer vocabulary. This tokenizer splits our whitespace separated words further into in dictionary sub-word pieces. The model eventually uses the label from the first piece of a word as the sole label for the word, so we do not care about the model's ability to predict individual labels for the sub-word pieces. For training, the label used for these pieces is `X`. To learn more see the [BERT paper](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_labeler(log_list, label_list):\n",
    "    \"\"\"\n",
    "    label all subword pieces in tokenized log with an 'X'\n",
    "    \"\"\"\n",
    "    subword_labels = []\n",
    "    for log, tags in zip(log_list,label_list):\n",
    "        temp_tags = []\n",
    "        words = cudf.Series(log.split())\n",
    "        words_size = len(words)\n",
    "        subword_counts = words.str.subword_tokenize(\"resources/bert-base-cased-hash.txt\", 10000, 10000,\\\n",
    "                                                    max_rows_tensor=words_size,\\\n",
    "                                                    do_lower=False, do_truncate=False)[2].reshape(words_size, 3)[:,2]\n",
    "        for i, tag in enumerate(tags):\n",
    "            temp_tags.append(tag)\n",
    "            temp_tags.extend('X'* subword_counts[i].item())\n",
    "        subword_labels.append(temp_tags)\n",
    "    return subword_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_labels = subword_labeler(logs_df.raw_preprocess.to_arrow().to_pylist(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['remote_host', 'X', 'X', 'X', 'X', 'X', 'X', 'other', 'other', 'time_received', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'time_received', 'X', 'X', 'X', 'request_method', 'X', 'request_url', 'other', 'X', 'X', 'X', 'X', 'X', 'X', 'other', 'response_bytes_clf', 'X', 'other', 'request_header_user_agent', 'X', 'X', 'X', 'X', 'X', 'request_header_user_agent', 'X', 'X', 'request_header_user_agent', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'request_header_user_agent', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'other']\n"
     ]
    }
   ],
   "source": [
    "print(subword_labels[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a set list of all labels from our dataset, add `X` for wordpiece tokens we will not have tags for and `[PAD]` for logs shorter than the length of the model's embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of labels\n",
    "label_values = list(set(x for l in labels for x in l))\n",
    "\n",
    "label_values[:0] = ['[PAD]']  \n",
    "label_values.append('X')\n",
    "\n",
    "# Set a dict for mapping id to tag name\n",
    "label2idx = {t: i for i, t in enumerate(label_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, 'time_received': 1, 'error_level': 2, 'error_message': 3, 'request_header_user_agent': 4, 'request_header_referer': 5, 'request_url': 6, 'request_method': 7, 'remote_host': 8, 'response_bytes_clf': 9, 'other': 10, 'X': 11}\n"
     ]
    }
   ],
   "source": [
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(l, content, width):\n",
    "    l.extend([content] * (width - len(l)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_labels = [pad(x[:256], '[PAD]', 256) for x in subword_labels]\n",
    "int_labels = [[label2idx.get(l) for l in lab] for lab in padded_labels]\n",
    "label_tensor = torch.tensor(int_labels).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Datasets\n",
    "For training and validation our datasets need three features. (1) `input_ids` subword tokens as integers padded to the specific length of the model (2) `attention_mask` a binary mask that allows the model to ignore padding (3) `labels` corresponding labels for tokens as integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_cased_tokenizer(strings):\n",
    "    \"\"\"\n",
    "    converts cudf.Seires of strings to two torch tensors- token ids and attention mask with padding\n",
    "    \"\"\"    \n",
    "    num_strings = len(strings)\n",
    "    num_bytes = strings.str.byte_count().sum()\n",
    "    token_ids, mask = strings.str.subword_tokenize(\"resources/bert-base-cased-hash.txt\", 256, 256,\n",
    "                                                            max_rows_tensor=num_strings,\n",
    "                                                            do_lower=False, do_truncate=True)[:2]\n",
    "    # convert from cupy to torch tensor using dlpack\n",
    "    input_ids = from_dlpack(token_ids.reshape(num_strings,256).astype(cupy.float).toDlpack())\n",
    "    attention_mask = from_dlpack(mask.reshape(num_strings,256).astype(cupy.float).toDlpack())\n",
    "    return input_ids.type(torch.long), attention_mask.type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks = bert_cased_tokenizer(logs_df.raw_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch random_split to create training and validation data subsets\n",
    "dataset_size = len(input_ids)\n",
    "training_dataset, validation_dataset = random_split(dataset, (int(dataset_size*.8), int(dataset_size*.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "train_dataloader = DataLoader(dataset=training_dataset, shuffle=True, batch_size=32)\n",
    "val_dataloader = DataLoader(dataset=validation_dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning pretrained BERT\n",
    "Download pretrained model from HuggingFace and move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaf77c345b44137ab4e8f3d4c570e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ad77848c834f7b825d7979a5158173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435779157.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label2idx))\n",
    "\n",
    "# model to gpu\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer and learning rate for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    #fine tune all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 1/2 [00:12<00:12, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6824694395065307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [00:24<00:00, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.043616552650928495\n",
      "CPU times: user 24.4 s, sys: 67.8 ms, total: 24.4 s\n",
      "Wall time: 24.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# using 2 epochs to avoid overfitting\n",
    "\n",
    "epochs = 2\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss, scores = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no dropout or batch norm during eval\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.997831\n",
      "Accuracy score: 0.999475\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                    other     1.0000    1.0000    1.0000       619\n",
      "            time_received     1.0000    1.0000    1.0000       200\n",
      "              remote_host     1.0000    1.0000    1.0000       182\n",
      "              request_url     1.0000    1.0000    1.0000       182\n",
      "           request_method     1.0000    1.0000    1.0000       182\n",
      "       response_bytes_clf     1.0000    1.0000    1.0000       181\n",
      "request_header_user_agent     0.9763    0.9880    0.9821       167\n",
      "   request_header_referer     1.0000    1.0000    1.0000        93\n",
      "            error_message     1.0000    1.0000    1.0000        18\n",
      "              error_level     1.0000    1.0000    1.0000        18\n",
      "\n",
      "                micro avg     0.9967    0.9989    0.9978      1842\n",
      "                macro avg     0.9979    0.9989    0.9984      1842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mapping index to name\n",
    "idx2label={label2idx[key] : key for key in label2idx.keys()}\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for step, batch in enumerate(val_dataloader):\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask,)\n",
    "        \n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "        \n",
    "    # Get NER predicted result\n",
    "    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.detach().cpu().numpy()\n",
    "    \n",
    "    # Only predict the groud truth, mask=0, will not calculate\n",
    "    input_mask = input_mask.detach().cpu().numpy()\n",
    "    \n",
    "    # Compare the valuable predict result\n",
    "    for i,mask in enumerate(input_mask):\n",
    "        # ground truth \n",
    "        temp_1 = []\n",
    "        # Prediction\n",
    "        temp_2 = []\n",
    "        \n",
    "        for j, m in enumerate(mask):\n",
    "            # Mask=0 is PAD, do not compare\n",
    "            if m: # Exclude the X label\n",
    "                if idx2label[label_ids[i][j]] != \"X\" and idx2label[label_ids[i][j]] != \"[PAD]\": \n",
    "                    temp_1.append(idx2label[label_ids[i][j]])\n",
    "                    temp_2.append(idx2label[logits[i][j]])\n",
    "            else:\n",
    "                break      \n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "\n",
    "print(\"f1 score: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "# Get acc , recall, F1 result report\n",
    "print(classification_report(y_true, y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model files for future parsing with cyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model weights \n",
    "#torch.save(model.state_dict(), 'path/to/save.pth')\n",
    "\n",
    "# label map\n",
    "#with open('path/to/save.pth', mode=\"wt\") as output:\n",
    "#    for label in idx2label.values():\n",
    "#        output.writelines(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}