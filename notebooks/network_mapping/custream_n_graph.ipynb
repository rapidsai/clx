{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fallen-bulgarian",
   "metadata": {},
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "roman-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "from cugraph.utilities.utils import is_device_version_less_than\n",
    "import pandas as pd\n",
    "\n",
    "from clx.heuristics import ports\n",
    "import clx.parsers.zeek as zeek\n",
    "import clx.ip\n",
    "\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import s3fs\n",
    "from streamz import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\"\n",
    "CONN_LOG = \"conn.log\"\n",
    "\n",
    "# Download Zeek conn log\n",
    "if not path.exists(CONN_LOG):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + CONN_LOG, CONN_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-electron",
   "metadata": {},
   "source": [
    "Note, `conn.log` contains a header at the top of the file, which is not needed for this example and we can simply remove it. It also contains a `close` header at the bottom, which we can remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n +9 conn.log | head -n -1 > messages.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-container",
   "metadata": {},
   "source": [
    "### Following the instructions at https://kafka.apache.org/quickstart to start a Kafka broker\n",
    "\n",
    "**NOTE:** At the topic creation step, make sure to name the new topic `streamz_n_graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting data into kafka\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streamz_n_graph < messages.log >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the data from the kafka topic\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-console-consumer.sh --topic streamz_n_graph --from-beginning --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-accreditation",
   "metadata": {},
   "source": [
    "### Configuring Kafka Stream using custreamz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handmade-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka\n",
    "broker=\"localhost:9092\"\n",
    "input_topic=\"streamz_n_graph\"\n",
    "output_topic=\"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "expected-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size=100000\n",
    "poll_interval=\"1s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specific-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate a unique group_id to be able to re-run this demo notebook on the same data loaded to your kafka topic.\n",
    "j = random.randint(0,10000)\n",
    "group_id=\"fil-group-%d\" % j\n",
    "\n",
    "# Kafka consumer configuration\n",
    "consumer_conf = {\n",
    "    \"bootstrap.servers\": broker,\n",
    "    \"group.id\": group_id,\n",
    "    \"session.timeout.ms\": \"60000\",\n",
    "    \"enable.partition.eof\": \"true\",\n",
    "    \"auto.offset.reset\": \"latest\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "identified-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = Stream.from_kafka_batched(\n",
    "        input_topic,\n",
    "        consumer_conf,\n",
    "        poll_interval=poll_interval,\n",
    "        npartitions=1,\n",
    "        asynchronous=True,\n",
    "        max_batch_size=max_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-terry",
   "metadata": {},
   "source": [
    "### Now we know that Kafka is setup correctly, we start customizing our `predict` function for clx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "answering-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def parse_message(line):\n",
    "    split_line = line.split(b'\\t')\n",
    "    src, src_p = split_line[2], split_line[3]\n",
    "    dest, dest_p = split_line[4], split_line[5]\n",
    "    return (src, src_p, dest, dest_p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_gdf = None\n",
    "\n",
    "\n",
    "def process_batch(messages):\n",
    "    global edges_gdf\n",
    "    start_time = time.time()\n",
    "    src_dest_tuples = list(map(parse_message, messages))\n",
    "    \n",
    "    evt_edges_df = cudf.DataFrame({\n",
    "        'src': [x[0].decode('utf-8') for x in src_dest_tuples],\n",
    "        'dst': [x[2].decode('utf-8') for x in src_dest_tuples]\n",
    "    })\n",
    "    \n",
    "    # converting to ip\n",
    "    evt_edges_df['src'] = clx.ip.ip_to_int(evt_edges_df['src'])\n",
    "    evt_edges_df['dst'] = clx.ip.ip_to_int(evt_edges_df['dst'])\n",
    "    \n",
    "    if not edges_gdf:\n",
    "        edges_gdf = evt_edges_df\n",
    "    else:\n",
    "        edges_gdf = cudf.concat([edges_gdf, evt_edges_df])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    return (time_diff, evt_edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(message):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(edges_gdf, source=\"src\", destination=\"dst\", renumber=True)    \n",
    "    \n",
    "    pr_gdf = cugraph.pagerank(G, alpha=0.85, max_iter=500, tol=1.0e-05)\n",
    "    pr_gdf['idx'] = pr_gdf['vertex']\n",
    "    \n",
    "    print(pr_gdf.head())\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    \n",
    "    prev_time = message[0]\n",
    "    return (prev_time, time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-annex",
   "metadata": {},
   "source": [
    "### Sinking the result to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = source.map(process_batch).map(pagerank).sink_to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-screening",
   "metadata": {},
   "source": [
    "### Generating longer synthetic file from `messages.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = open('messages.log').read()\n",
    "factor = 46\n",
    "messages_sent = 43410 * factor  # 46 * 43410 ~ 2 million\n",
    "\n",
    "with open('messages_duplicate.log', 'w') as f:\n",
    "    for i in range(factor):\n",
    "        f.write(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-forward",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cumulative_time, total_time = 0, 0\n",
    "trials = 10\n",
    "bashCommand = \"kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streamz_n_graph < messages_duplicate.log >/dev/null\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trials):\n",
    "    process = subprocess.Popen(bashCommand, stdout=subprocess.PIPE, cwd='/rapids/clx/my_data', shell=True)\n",
    "    process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A total of {messages_sent*trials} messages will be sent')\n",
    "\n",
    "if len(output)*max_batch_size >= messages_sent*trials:\n",
    "    print('Done')\n",
    "    print('Average seconds per message:', sum(x[0] + x[1] for x in output)/(messages_sent * trials))\n",
    "else:\n",
    "    print('Still running, current average seconds per message:', sum(x[0] + x[1] for x in output)/(messages_sent * trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-meter",
   "metadata": {},
   "source": [
    "### Sinking the result to Kafka\n",
    "\n",
    "Instead of sinking to a list, we can also emit our edge-list/pagerank result to a kafka topic, we just need to convert our result to a string or byte object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norman-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker=\"localhost:9092\"\n",
    "input_topic=\"streamz_n_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neither-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size=5000\n",
    "poll_interval=\"1s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "final-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate a unique group_id to be able to re-run this demo notebook on the same data loaded to your kafka topic.\n",
    "j = random.randint(0,10000)\n",
    "group_id=\"fil-group-%d\" % j\n",
    "\n",
    "# Kafka consumer configuration\n",
    "consumer_conf = {\n",
    "    \"bootstrap.servers\": broker,\n",
    "    \"group.id\": group_id,\n",
    "    \"session.timeout.ms\": \"60000\",\n",
    "    \"enable.partition.eof\": \"true\",\n",
    "    \"auto.offset.reset\": \"latest\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "equipped-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = Stream.from_kafka_batched(\n",
    "        input_topic,\n",
    "        consumer_conf,\n",
    "        poll_interval=poll_interval,\n",
    "        npartitions=1,\n",
    "        asynchronous=True,\n",
    "        max_batch_size=max_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the two new topics\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-topics.sh --create --topic edge_list --bootstrap-server localhost:9092\n",
    "!kafka_2.13-2.8.0/bin/kafka-topics.sh --create --topic pagerank --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controversial-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_message(line):\n",
    "    split_line = line.split(b'\\t')\n",
    "    src, src_p = split_line[2], split_line[3]\n",
    "    dest, dest_p = split_line[4], split_line[5]\n",
    "    return (src, src_p, dest, dest_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "motivated-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_gdf = None\n",
    "\n",
    "def process_batch(messages):\n",
    "    global edges_gdf\n",
    "    src_dest_tuples = list(map(parse_message, messages))\n",
    "    \n",
    "    evt_edges_df = cudf.DataFrame({\n",
    "        'src': [x[0].decode('utf-8') for x in src_dest_tuples],\n",
    "        'dst': [x[2].decode('utf-8') for x in src_dest_tuples]\n",
    "    })\n",
    "    \n",
    "    # converting to ip\n",
    "    evt_edges_df['src'] = clx.ip.ip_to_int(evt_edges_df['src'])\n",
    "    evt_edges_df['dst'] = clx.ip.ip_to_int(evt_edges_df['dst'])\n",
    "    \n",
    "    if not edges_gdf:\n",
    "        edges_gdf = evt_edges_df\n",
    "    else:\n",
    "        edges_gdf = cudf.concat([edges_gdf, evt_edges_df])\n",
    "\n",
    "    return evt_edges_df.to_json(orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civil-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(messages):\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(edges_gdf, source=\"src\", destination=\"dst\", renumber=True)    \n",
    "    \n",
    "    pr_gdf = cugraph.pagerank(G, alpha=0.85, max_iter=500, tol=1.0e-05)\n",
    "    pr_gdf['idx'] = pr_gdf['vertex']\n",
    "    \n",
    "    return pr_gdf.to_json(orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incorporated-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = {'bootstrap.servers': 'localhost:9092'}\n",
    "output = source.map(process_batch).to_kafka('edge_list', ARGS).map(pagerank).to_kafka('pagerank', ARGS).sink_to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vulnerable-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting data into kafka\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streamz_n_graph < messages.log >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run below to see messages sent to output\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-console-consumer.sh --topic edge_list --from-beginning --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kafka_2.13-2.8.0/bin/kafka-console-consumer.sh --topic pagerank --from-beginning --bootstrap-server localhost:9092"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
