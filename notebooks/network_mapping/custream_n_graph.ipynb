{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automotive-association",
   "metadata": {},
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "amazing-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "from cugraph.utilities.utils import is_device_version_less_than\n",
    "import pandas as pd\n",
    "\n",
    "from clx.heuristics import ports\n",
    "import clx.parsers.zeek as zeek\n",
    "import clx.ip\n",
    "\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import s3fs\n",
    "from streamz import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "technological-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\"\n",
    "CONN_LOG = \"conn.log\"\n",
    "\n",
    "# Download Zeek conn log\n",
    "if not path.exists(CONN_LOG):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + CONN_LOG, CONN_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-radical",
   "metadata": {},
   "source": [
    "Note, `conn.log` contains a header at the top of the file, which is not needed for this example and we can simply remove it. It also contains a `close` header at the bottom, which we can remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "desirable-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n +9 conn.log | head -n -1 > messages.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-chase",
   "metadata": {},
   "source": [
    "### Following the instructions at https://kafka.apache.org/quickstart to start a Kafka broker\n",
    "\n",
    "**NOTE:** At the topic creation step, make sure to name the new topic `streamz_n_graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting data into kafka\n",
    "\n",
    "!kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streamz_n_graph < messages.log >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the data from the kafka topic\n",
    "\n",
    "!kafka/bin/kafka-console-consumer.sh --topic streamz_n_graph --from-beginning --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-match",
   "metadata": {},
   "source": [
    "### Configuring Kafka Stream using custreamz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "informal-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka\n",
    "broker=\"localhost:9092\"\n",
    "input_topic=\"streamz_n_graph\"\n",
    "output_topic=\"output\"\n",
    "\n",
    "producer_conf = {\n",
    "    \"bootstrap.servers\": broker,\n",
    "    \"session.timeout.ms\": 10000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "headed-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size=100000\n",
    "poll_interval=\"1s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "systematic-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# WHAT DOES THIS MEAN?\n",
    "# Generate a unique group_id to be able to re-run this demo notebook on the same data loaded to your kafka topic.\n",
    "j = random.randint(0,10000)\n",
    "group_id=\"fil-group-%d\" % j\n",
    "\n",
    "# Kafka consumer configuration\n",
    "consumer_conf = {\n",
    "    \"bootstrap.servers\": broker,\n",
    "    \"group.id\": group_id,\n",
    "    \"session.timeout.ms\": \"60000\",\n",
    "    \"enable.partition.eof\": \"true\",\n",
    "    \"auto.offset.reset\": \"latest\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "offshore-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = Stream.from_kafka_batched(\n",
    "        input_topic,\n",
    "        consumer_conf,\n",
    "        poll_interval=poll_interval,\n",
    "        npartitions=1,\n",
    "        asynchronous=True,\n",
    "        max_batch_size=max_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-boxing",
   "metadata": {},
   "source": [
    "### Now we know that Kafka is setup correctly, we start customizing our `predict` function for clx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "urban-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def parse_message(line):\n",
    "    split_line = line.split(b'\\t')\n",
    "    src, src_p = split_line[2], split_line[3]\n",
    "    dest, dest_p = split_line[4], split_line[5]\n",
    "    return (src, src_p, dest, dest_p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ideal-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_gdf = None\n",
    "\n",
    "\n",
    "def process_batch(messages):\n",
    "    global edges_gdf\n",
    "    start_time = time.time()\n",
    "    src_dest_tuples = list(map(parse_message, messages))\n",
    "    \n",
    "    # single pass\n",
    "    evt_edges_df = cudf.DataFrame({\n",
    "        'src': [x[0].decode('utf-8') for x in src_dest_tuples],\n",
    "        'dst': [x[2].decode('utf-8') for x in src_dest_tuples]\n",
    "    })\n",
    "    \n",
    "    # converting to ip\n",
    "    evt_edges_df['src'] = clx.ip.ip_to_int(evt_edges_df['src'])\n",
    "    evt_edges_df['dst'] = clx.ip.ip_to_int(evt_edges_df['dst'])\n",
    "    \n",
    "    if not edges_gdf:\n",
    "        edges_gdf = evt_edges_df\n",
    "    else:\n",
    "        edges_gdf = cudf.concat([edges_gdf, evt_edges_df])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    return (time_diff, evt_edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respective-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(message):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(edges_gdf, source=\"src\", destination=\"dst\", renumber=True)    \n",
    "    \n",
    "    pr_gdf = cugraph.pagerank(G, alpha=0.85, max_iter=500, tol=1.0e-05)\n",
    "    pr_gdf['idx'] = pr_gdf['vertex']\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    \n",
    "    prev_time = message[0]\n",
    "    return (prev_time, time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "graphic-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = source.map(process_batch).map(pagerank).sink_to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prospective-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-vegetation",
   "metadata": {},
   "source": [
    "### Generating longer synthetic file from `messages.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "olympic-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = open('messages.log').read()\n",
    "factor = 46\n",
    "messages_sent = 43410 * factor  # 46 * 43410 ~ 2 million\n",
    "\n",
    "with open('messages_duplicate.log', 'w') as f:\n",
    "    for i in range(factor):\n",
    "        f.write(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-arctic",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "applicable-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cumulative_time, total_time = 0, 0\n",
    "trials = 10\n",
    "bashCommand = \"kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streamz_n_graph < messages_duplicate.log >/dev/null\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "involved-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trials):\n",
    "    process = subprocess.Popen(bashCommand, stdout=subprocess.PIPE, cwd='/rapids/clx/my_data', shell=True)\n",
    "    process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A total of {messages_sent*trials} messages will be sent')\n",
    "\n",
    "if len(output)*max_batch_size >= messages_sent*trials:\n",
    "    print('Done')\n",
    "    print('Average seconds per message:', sum(x[0] + x[1] for x in output)/(messages_sent * trials))\n",
    "else:\n",
    "    print('Still running, current average seconds per message:', sum(x[0] + x[1] for x in output)/(messages_sent * trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-passport",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
