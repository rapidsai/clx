{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ce4a75-08a2-48d3-bc5d-674046efd620",
   "metadata": {},
   "source": [
    "## **Sample workflow using cugraph, custreamz and clx**\n",
    "\n",
    "## Authors:\n",
    "- Shane Ding (NVIDIA) [shaned@nvidia.com]\n",
    "\n",
    "## Development Notes\n",
    "* Developed using: CLX v0.18 and RAPIDS v0.18.0\n",
    "* Last tested using: CLX v0.18 and RAPIDS v0.18.0 on June 9th, 2021\n",
    "\n",
    "## Table of Contents\n",
    "- Downloading Data\n",
    "- Starting Kafka\n",
    "- Configuring Kafka\n",
    "- Building custreamz pipeline\n",
    "- Benchmarking\n",
    "- Publishing results to Kafka\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we show an example of a workflow wherein data is published to Kafka, which is then processed via [RAPIDS](https://rapids.ai/) (in particular `cudf`, `cugraph` and `custreamz`) and [CLX](https://github.com/rapidsai/clx) for graph analytic workflows. The data we use is a sample from the [UNSW-NB15 dataset](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/) which can be downloaded [here](https://cloudstor.aarnet.edu.au/plus/index.php/s/2DhnLGDdEECo4ys) or simply run the blocks below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-bulgarian",
   "metadata": {},
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "from cugraph.utilities.utils import is_device_version_less_than\n",
    "import pandas as pd\n",
    "\n",
    "from clx.heuristics import ports\n",
    "import clx.parsers.zeek as zeek\n",
    "import clx.ip\n",
    "\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import s3fs\n",
    "from streamz import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\"\n",
    "CONN_LOG = \"conn.log\"\n",
    "\n",
    "# Download Zeek conn log\n",
    "if not path.exists(CONN_LOG):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + CONN_LOG, CONN_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-electron",
   "metadata": {},
   "source": [
    "Note, `conn.log` contains a header at the top of the file, which is not needed for this example and we can simply remove it. It also contains a `close` header at the bottom, which we can remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n +9 conn.log | head -n -1 > messages.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-container",
   "metadata": {},
   "source": [
    "### Following the instructions at https://kafka.apache.org/quickstart to start a Kafka broker\n",
    "\n",
    "**NOTE:** At the topic creation step, make sure to name the new topic `streamz_n_graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting data into kafka\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streamz_n_graph < messages.log >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the data from the kafka topic\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-console-consumer.sh --topic streamz_n_graph --from-beginning --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-accreditation",
   "metadata": {},
   "source": [
    "### Configuring Kafka Stream using custreamz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka\n",
    "broker=\"localhost:9092\"\n",
    "input_topic=\"streamz_n_graph\"\n",
    "output_topic=\"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size=100000\n",
    "poll_interval=\"1s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate a unique group_id to be able to re-run this demo notebook on the same data loaded to your kafka topic.\n",
    "j = random.randint(0,10000)\n",
    "group_id=\"fil-group-%d\" % j\n",
    "\n",
    "# Kafka consumer configuration\n",
    "consumer_conf = {\n",
    "    \"bootstrap.servers\": broker,\n",
    "    \"group.id\": group_id,\n",
    "    \"session.timeout.ms\": \"60000\",\n",
    "    \"enable.partition.eof\": \"true\",\n",
    "    \"auto.offset.reset\": \"latest\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = Stream.from_kafka_batched(\n",
    "        input_topic,\n",
    "        consumer_conf,\n",
    "        poll_interval=poll_interval,\n",
    "        npartitions=1,\n",
    "        asynchronous=True,\n",
    "        max_batch_size=max_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-terry",
   "metadata": {},
   "source": [
    "### Now we know that Kafka is setup correctly, we start customizing our `predict` function for clx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def parse_message(line):\n",
    "    split_line = line.split(b'\\t')\n",
    "    src, src_p = split_line[2], split_line[3]\n",
    "    dest, dest_p = split_line[4], split_line[5]\n",
    "    return (src, src_p, dest, dest_p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_gdf = None\n",
    "\n",
    "\n",
    "def process_batch(messages):\n",
    "    global edges_gdf\n",
    "    start_time = time.time()\n",
    "    src_dest_tuples = list(map(parse_message, messages))\n",
    "    \n",
    "    evt_edges_df = cudf.DataFrame({\n",
    "        'src': [x[0].decode('utf-8') for x in src_dest_tuples],\n",
    "        'dst': [x[2].decode('utf-8') for x in src_dest_tuples]\n",
    "    })\n",
    "    \n",
    "    # converting to ip\n",
    "    evt_edges_df['src'] = clx.ip.ip_to_int(evt_edges_df['src'])\n",
    "    evt_edges_df['dst'] = clx.ip.ip_to_int(evt_edges_df['dst'])\n",
    "    \n",
    "    if not edges_gdf:\n",
    "        edges_gdf = evt_edges_df\n",
    "    else:\n",
    "        edges_gdf = cudf.concat([edges_gdf, evt_edges_df])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    return (time_diff, evt_edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(message):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(edges_gdf, source=\"src\", destination=\"dst\", renumber=True)    \n",
    "    \n",
    "    pr_gdf = cugraph.pagerank(G, alpha=0.85, max_iter=500, tol=1.0e-05)\n",
    "    pr_gdf['idx'] = pr_gdf['vertex']\n",
    "    \n",
    "    print(pr_gdf.head())\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    \n",
    "    prev_time = message[0]\n",
    "    return (prev_time, time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-annex",
   "metadata": {},
   "source": [
    "### Sinking the result to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = source.map(process_batch).map(pagerank).sink_to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-screening",
   "metadata": {},
   "source": [
    "### Generating longer synthetic file from `messages.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = open('messages.log').read()\n",
    "factor = 46\n",
    "messages_sent = 43410 * factor  # 46 * 43410 ~ 2 million\n",
    "\n",
    "with open('messages_duplicate.log', 'w') as f:\n",
    "    for i in range(factor):\n",
    "        f.write(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-forward",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cumulative_time, total_time = 0, 0\n",
    "trials = 10\n",
    "bashCommand = \"kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streamz_n_graph < messages_duplicate.log >/dev/null\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trials):\n",
    "    process = subprocess.Popen(bashCommand, stdout=subprocess.PIPE, cwd='/rapids/clx/my_data', shell=True)\n",
    "    process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A total of {messages_sent*trials} messages will be sent')\n",
    "\n",
    "if len(output)*max_batch_size >= messages_sent*trials:\n",
    "    print('Done')\n",
    "    print('Average seconds per message:', sum(x[0] + x[1] for x in output)/(messages_sent * trials))\n",
    "else:\n",
    "    print('Still running, current average seconds per message:', sum(x[0] + x[1] for x in output)/(messages_sent * trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-meter",
   "metadata": {},
   "source": [
    "### Publishing the results to Kafka\n",
    "\n",
    "Instead of sinking to a list, we can also emit our edge-list/pagerank result to a kafka topic, we just need to convert our result to a string or byte object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker=\"localhost:9092\"\n",
    "input_topic=\"streamz_n_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size=5000\n",
    "poll_interval=\"1s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate a unique group_id to be able to re-run this demo notebook on the same data loaded to your kafka topic.\n",
    "j = random.randint(0,10000)\n",
    "group_id=\"fil-group-%d\" % j\n",
    "\n",
    "# Kafka consumer configuration\n",
    "consumer_conf = {\n",
    "    \"bootstrap.servers\": broker,\n",
    "    \"group.id\": group_id,\n",
    "    \"session.timeout.ms\": \"60000\",\n",
    "    \"enable.partition.eof\": \"true\",\n",
    "    \"auto.offset.reset\": \"latest\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = Stream.from_kafka_batched(\n",
    "        input_topic,\n",
    "        consumer_conf,\n",
    "        poll_interval=poll_interval,\n",
    "        npartitions=1,\n",
    "        asynchronous=True,\n",
    "        max_batch_size=max_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the two new topics\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-topics.sh --create --topic edge_list --bootstrap-server localhost:9092\n",
    "!kafka_2.13-2.8.0/bin/kafka-topics.sh --create --topic pagerank --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_message(line):\n",
    "    split_line = line.split(b'\\t')\n",
    "    src, src_p = split_line[2], split_line[3]\n",
    "    dest, dest_p = split_line[4], split_line[5]\n",
    "    return (src, src_p, dest, dest_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_gdf = None\n",
    "\n",
    "def process_batch(messages):\n",
    "    global edges_gdf\n",
    "    src_dest_tuples = list(map(parse_message, messages))\n",
    "    \n",
    "    evt_edges_df = cudf.DataFrame({\n",
    "        'src': [x[0].decode('utf-8') for x in src_dest_tuples],\n",
    "        'dst': [x[2].decode('utf-8') for x in src_dest_tuples]\n",
    "    })\n",
    "    \n",
    "    # converting to ip\n",
    "    evt_edges_df['src'] = clx.ip.ip_to_int(evt_edges_df['src'])\n",
    "    evt_edges_df['dst'] = clx.ip.ip_to_int(evt_edges_df['dst'])\n",
    "    \n",
    "    if not edges_gdf:\n",
    "        edges_gdf = evt_edges_df\n",
    "    else:\n",
    "        edges_gdf = cudf.concat([edges_gdf, evt_edges_df])\n",
    "\n",
    "    return evt_edges_df.to_json(orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(messages):\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(edges_gdf, source=\"src\", destination=\"dst\", renumber=True)    \n",
    "    \n",
    "    pr_gdf = cugraph.pagerank(G, alpha=0.85, max_iter=500, tol=1.0e-05)\n",
    "    pr_gdf['idx'] = pr_gdf['vertex']\n",
    "    \n",
    "    return pr_gdf.to_json(orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = {'bootstrap.servers': 'localhost:9092'}\n",
    "output = source.map(process_batch).to_kafka('edge_list', ARGS).map(pagerank).to_kafka('pagerank', ARGS).sink_to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68f4a9-6e3d-422f-954e-38f8258198db",
   "metadata": {},
   "source": [
    "#### Copy the two commands below and run in new windows to see the messages published to the `edge_list` and `pagerank` topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run below to see messages sent to output\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-console-consumer.sh --topic edge_list --from-beginning --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kafka_2.13-2.8.0/bin/kafka-console-consumer.sh --topic pagerank --from-beginning --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11b65c-1f5e-428c-9912-6548fad7959f",
   "metadata": {},
   "source": [
    "#### Publishing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting data into kafka\n",
    "\n",
    "!kafka_2.13-2.8.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streamz_n_graph < messages.log >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbf058-41aa-4fe1-b816-dd4d87becdef",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca35a52-b343-4eea-b1d4-2f2768126af8",
   "metadata": {},
   "source": [
    "In this notebook, we have shown how CLX and RAPIDS can be used together for real-time graph analytics use cases, wherein speed and processing power is extremely important. Further addition to this work may include exploring ways we can generalize the graph creation process across message types and also running more complex analysis on the graph created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c110200-d570-47c5-9a4f-00450fac705f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
