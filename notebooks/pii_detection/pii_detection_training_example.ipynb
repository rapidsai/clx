{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fiscal-edward",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT language model for PII labeling\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Load training dataset with cudf\n",
    "* Transform labels into pytorch tensor using dlpack\n",
    "* Transform text using cudf subword tokenizer\n",
    "* Split into train and test sets\n",
    "* Loading pretrained model\n",
    "* Fine-tune the model\n",
    "* Model evaluation\n",
    "* Save model file\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Detecting PII inside of text data is an arduous task, often requiring complex regex and heuristics. This notebook illustrates how to train a language model using a dataset of 1000 API responses that have been previously labeled as containing up to ten different types of PII. We will fine-tune a pretrained BERT model from [HuggingFace](https://github.com/huggingface) with a multi-label classification layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import s3fs\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from sklearn.metrics import f1_score, accuracy_score, multilabel_confusion_matrix\n",
    "from tqdm import trange\n",
    "import cudf\n",
    "import cupy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-knock",
   "metadata": {},
   "source": [
    "## Load training dataset with cudf\n",
    "\n",
    "To train our model we begin with a dataframe containing a field with text samples and one column for each of ten labels of PII. The label columns have either 0 or 1 for the presence of the specific PII type in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sample data\n",
    "PII_SAMPLE_CSV = \"pii_training_sample.csv\"\n",
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\"\n",
    "\n",
    "if not path.exists(PII_SAMPLE_CSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + PII_SAMPLE_CSV, PII_SAMPLE_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(PII_SAMPLE_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-scoop",
   "metadata": {},
   "source": [
    "## Transform labels into pytorch tensor using dlpack\n",
    "\n",
    "We find all the columns from the df that are labels for the text data and transform them into a tensor using dlpack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = list(df.columns)\n",
    "label_names.remove('text')\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = from_dlpack(df[label_names].to_dlpack()).type(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-egypt",
   "metadata": {},
   "source": [
    "## Transform text using cudf subword tokenizer\n",
    "\n",
    "We will define two tokenizers needed for two different models-- `bert-base-cased` using a pre-made vocab hash file `bert-base-cased-hash.txt`, and `mini-bert` using the hash file `bert-base-uncased-hash.txt`. Then we use one of our functions to transform the `text` column into two padded tensors for our model training-- `input_ids` and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer for bert-base-cased model\n",
    "\n",
    "def bert_cased_tokenizer(strings, seq_length):\n",
    "    \"\"\"\n",
    "    converts cudf.Seires of strings to two torch tensors- token ids and attention mask with padding\n",
    "    \"\"\"    \n",
    "    num_strings = len(strings)\n",
    "    token_ids, mask = strings.str.subword_tokenize(\"bert-base-cased-hash.txt\", seq_length, seq_length,\n",
    "                                                            max_rows_tensor=num_strings,\n",
    "                                                            do_lower=False, do_truncate=True)[:2]\n",
    "    # convert from cupy to torch tensor using dlpack\n",
    "    input_ids = from_dlpack(token_ids.reshape(num_strings,seq_length).astype(cupy.float).toDlpack())\n",
    "    attention_mask = from_dlpack(mask.reshape(num_strings,seq_length).astype(cupy.float).toDlpack())\n",
    "    return input_ids.type(torch.long), attention_mask.type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer for use with mini-bert or bert-base-uncased models\n",
    "\n",
    "def bert_uncased_tokenizer(strings, seq_length):\n",
    "    \"\"\"\n",
    "    converts cudf.Seires of strings to two torch tensors- token ids and attention mask with padding\n",
    "    \"\"\"    \n",
    "    num_strings = len(strings)\n",
    "    token_ids, mask = strings.str.subword_tokenize(\"bert-base-uncased-hash.txt\", seq_length, seq_length,\n",
    "                                                            max_rows_tensor=num_strings,\n",
    "                                                            do_lower=True, do_truncate=True)[:2]\n",
    "    # convert from cupy to torch tensor using dlpack\n",
    "    input_ids = from_dlpack(token_ids.reshape(num_strings,seq_length).astype(cupy.float).toDlpack())\n",
    "    attention_mask = from_dlpack(mask.reshape(num_strings,seq_length).astype(cupy.float).toDlpack())\n",
    "    return input_ids.type(torch.long), attention_mask.type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick model and tokenizer\n",
    "\n",
    "MODEL_NAME = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "TOKENIZER = bert_uncased_tokenizer\n",
    "\n",
    "# or choose bert-base-cased\n",
    "# MODEL_NAME = \"bert-base-cased\"\n",
    "# TOKENIZER = bert_cased_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input_ids and attention_masks tensors\n",
    "input_ids, attention_masks = TOKENIZER(df.text, 256) # using 256 for our model sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-australian",
   "metadata": {},
   "source": [
    "## Split into train and test sets\n",
    "\n",
    "Create at pytorch dataset, split into testing and training subsets, and load into pytorch dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# use pytorch random_split to create training and validation data subsets\n",
    "dataset_size = len(input_ids)\n",
    "train_size = int(dataset_size * .8) # 80/20 split\n",
    "training_dataset, validation_dataset = random_split(dataset, (train_size, (dataset_size-train_size)))\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(dataset=training_dataset, shuffle=True, batch_size=32)\n",
    "val_dataloader = DataLoader(dataset=validation_dataset, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-judge",
   "metadata": {},
   "source": [
    "## Load pretrained model from huggingface repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the following model for bert-base-cased\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                           num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.cuda(); # move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of gpus\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# use DataParallel if you have more than one GPU\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-segment",
   "metadata": {},
   "source": [
    "## Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using hyperparameters recommended in orginial BERT paper\n",
    "# the optimizer allows us to apply different hyperpameters for specific parameter groups\n",
    "# apply weight decay to all parameters other than bias, gamma, and beta\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training epochs\n",
    "epochs = 4\n",
    "\n",
    "# train loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  # tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # train the data for one epoch\n",
    "    for batch in train_dataloader:\n",
    "        # unpack the inputs from dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # clear out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # using binary cross-entropy with logits as loss function\n",
    "        # assigns independent probabilities to each label\n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation \n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu parallel training\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-migration",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We evaluate the accuracy on the 20% of data we have in the validation set. We report the `F1 macro accuracy`- correct_predictions divided by total_predictions is calculated for each label and averaged, and the `flat accuracy`- correct_predictions divided by total_predctions of the model for the validation set as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to eval mode to evaluate loss on the validation set\n",
    "model.eval()\n",
    "\n",
    "# variables to gather full output\n",
    "logit_preds,true_labels,pred_labels = [],[],[]\n",
    "\n",
    "# predict\n",
    "for batch in val_dataloader:\n",
    "    # unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        # forward pass\n",
    "        output = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        b_logit_pred = output[0]\n",
    "        b_pred_label = torch.sigmoid(b_logit_pred)\n",
    "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "        b_pred_label = b_pred_label.detach().cpu().numpy()\n",
    "        b_labels = b_labels.detach().cpu().numpy()\n",
    "    \n",
    "    logit_preds.extend(b_logit_pred)\n",
    "    true_labels.extend(b_labels)\n",
    "    pred_labels.extend(b_pred_label)\n",
    "\n",
    "# calculate accuracy, using 0.50 threshold\n",
    "threshold = 0.50\n",
    "pred_bools = [pl>threshold for pl in pred_labels]\n",
    "true_bools = [tl==1 for tl in true_labels]\n",
    "val_f1_accuracy = f1_score(true_bools,pred_bools,average='macro')*100\n",
    "val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "print('F1 Macro Validation Accuracy: ', val_f1_accuracy)\n",
    "print('Flat Validation Accuracy: ', val_flat_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for each label\n",
    "\n",
    "for label, cf in zip(label_names, multilabel_confusion_matrix(true_bools, pred_bools)):\n",
    "                     print(label)\n",
    "                     print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-country",
   "metadata": {},
   "source": [
    "## Save model file\n",
    "\n",
    "If we're using data parallel save model as module, so you can use it either inside or outside of a multi-gpu environment later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_gpu > 1:\n",
    "    torch.save(model.module.state_dict(), \"path/to/your-model-name.pth\")\n",
    "else:\n",
    "    torch.save(model.state_dict(), \"path/to/your-model-name.pth\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-study",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using pretrained BERT models (`bert-base-cased` or `mini-bert`) from the huggingface repo and a custom traning for multi-label classification, we are able to successfully train a PII detector from our training dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
