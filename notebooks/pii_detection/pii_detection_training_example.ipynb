{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graphic-italian",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT language model for PII labeling\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Load training dataset with cudf\n",
    "* Transform labels into pytorch tensor using dlpack\n",
    "* Transform text using cudf subword tokenizer\n",
    "* Split into train and test sets\n",
    "* Loading pretrained model\n",
    "* Fine-tune the model\n",
    "* Model evaluation\n",
    "* Save model file\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Detecting PII inside of text data is an arduous task, often requiring complex regex and heuristics. This notebook illustrates how to train a language model using a dataset of 1000 API responses that have been previously labeled as containing up to ten different types of PII. We will fine-tune a pretrained BERT model from [HuggingFace](https://github.com/huggingface) with a multi-label classification layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "personalized-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import s3fs\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from sklearn.metrics import f1_score, accuracy_score, multilabel_confusion_matrix\n",
    "from tqdm import trange\n",
    "import cudf\n",
    "import cupy\n",
    "from cudf.core.subword_tokenizer import SubwordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-soccer",
   "metadata": {},
   "source": [
    "## Load training dataset with cudf\n",
    "\n",
    "To train our model we begin with a dataframe containing a field with text samples and one column for each of ten labels of PII. The label columns have either 0 or 1 for the presence of the specific PII type in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wooden-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sample data\n",
    "PII_SAMPLE_CSV = \"pii_training_sample.csv\"\n",
    "S3_BASE_PATH = \"rapidsai-data/cyber/pii\"\n",
    "\n",
    "if not path.exists(PII_SAMPLE_CSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + PII_SAMPLE_CSV, PII_SAMPLE_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "limited-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(PII_SAMPLE_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-collective",
   "metadata": {},
   "source": [
    "## Transform labels into pytorch tensor using dlpack\n",
    "\n",
    "We find all the columns from the df that are labels for the text data and transform them into a tensor using dlpack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "victorian-blade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address',\n",
       " 'bank_acct',\n",
       " 'credit_card',\n",
       " 'email',\n",
       " 'govt_id',\n",
       " 'name',\n",
       " 'password',\n",
       " 'phone_num',\n",
       " 'secret_keys',\n",
       " 'user']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = list(df.columns)\n",
    "label_names.remove('text')\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continental-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = from_dlpack(df[label_names].to_dlpack()).type(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-navigator",
   "metadata": {},
   "source": [
    "## Transform text using cudf subword tokenizer\n",
    "\n",
    "We can define two tokenizers needed for two different models-- `bert-base-cased` using a pre-made vocab hash file `bert-base-cased-hash.txt`, and `mini-bert` using the hash file `bert-base-uncased-hash.txt`.\n",
    "Then we use one of our functions to transform the `text` column into two padded tensors for our model training-- `input_ids` and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73541867-08bf-445f-a728-4ffa09c5ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer for bert-base-cased model\n",
    "def tokenizer_func(tokenizer, strings, seq_length):\n",
    "    \"\"\"\n",
    "    converts cudf.Seires of strings to two torch tensors- token ids and attention mask with padding\n",
    "    \"\"\"    \n",
    "\n",
    "    output = tokenizer(strings,\n",
    "                   max_length=seq_length,\n",
    "                   max_num_rows=len(strings),\n",
    "                   truncation=True,\n",
    "                   add_special_tokens=False,\n",
    "                   return_tensors=\"pt\")\n",
    "\n",
    "    # convert from cupy to torch tensor using dlpack\n",
    "    return output['input_ids'].type(torch.long), output['attention_mask'].type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "educational-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick model and tokenizer\n",
    "\n",
    "MODEL_NAME = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "tokenizer = SubwordTokenizer('resources/bert-base-uncased-hash.txt', do_lower_case=True)\n",
    "# or choose bert-base-cased\n",
    "# MODEL_NAME = \"bert-base-cased\"\n",
    "# tokenizer = SubwordTokenizer('resources/bert-base-cased-hash.txt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01dc1766-b3e0-4194-a165-3d37da7ae99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks = tokenizer_func(tokenizer, df.text, 256) # using 256 for our model sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-richmond",
   "metadata": {},
   "source": [
    "## Split into train and test sets\n",
    "\n",
    "Create at pytorch dataset, split into testing and training subsets, and load into pytorch dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "broke-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# use pytorch random_split to create training and validation data subsets\n",
    "dataset_size = len(input_ids)\n",
    "train_size = int(dataset_size * .8) # 80/20 split\n",
    "training_dataset, validation_dataset = random_split(dataset, (train_size, (dataset_size-train_size)))\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(dataset=training_dataset, shuffle=True, batch_size=8)\n",
    "val_dataloader = DataLoader(dataset=validation_dataset, shuffle=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-implementation",
   "metadata": {},
   "source": [
    "## Load pretrained model from huggingface repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "desirable-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "raising-tiger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937d5a0dcd7d4fceb9cf592524bf8e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bfc5bcbeec46d29a95f3bd34ac599f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/43.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the following model for bert-base-cased\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                           num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "contrary-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.cuda(); # move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "advance-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of gpus\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# use DataParallel if you have more than one GPU\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-ridge",
   "metadata": {},
   "source": [
    "## Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "threatened-difficulty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# using hyperparameters recommended in orginial BERT paper\n",
    "# the optimizer allows us to apply different hyperpameters for specific parameter groups\n",
    "# apply weight decay to all parameters other than bias, gamma, and beta\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "persistent-watts",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██████████████████████████████████▌                                                                                                       | 1/4 [00:37<01:51, 37.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4860910549759865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████████████████████████████████████████████████████████████████████                                                                     | 2/4 [00:44<00:39, 19.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.35342139571905135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 3/4 [00:52<00:14, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.28969549730420113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:00<00:00, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.255231711268425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# number of training epochs\n",
    "epochs = 4\n",
    "\n",
    "# train loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  # tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # train the data for one epoch\n",
    "    for batch in train_dataloader:\n",
    "        # unpack the inputs from dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # clear out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # using binary cross-entropy with logits as loss function\n",
    "        # assigns independent probabilities to each label\n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation \n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu parallel training\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-spain",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We evaluate the accuracy on the 20% of data we have in the validation set. We report the `F1 macro accuracy`- correct_predictions divided by total_predictions is calculated for each label and averaged, and the `flat accuracy`- correct_predictions divided by total_predctions of the model for the validation set as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beautiful-gambling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Validation Accuracy:  4.313725490196079\n",
      "Flat Validation Accuracy:  47.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    }
   ],
   "source": [
    "# model to eval mode to evaluate loss on the validation set\n",
    "model.eval()\n",
    "\n",
    "# variables to gather full output\n",
    "logit_preds,true_labels,pred_labels = [],[],[]\n",
    "\n",
    "# predict\n",
    "for batch in val_dataloader:\n",
    "    # unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        # forward pass\n",
    "        output = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        b_logit_pred = output[0]\n",
    "        b_pred_label = torch.sigmoid(b_logit_pred)\n",
    "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "        b_pred_label = b_pred_label.detach().cpu().numpy()\n",
    "        b_labels = b_labels.detach().cpu().numpy()\n",
    "    \n",
    "    logit_preds.extend(b_logit_pred)\n",
    "    true_labels.extend(b_labels)\n",
    "    pred_labels.extend(b_pred_label)\n",
    "\n",
    "# calculate accuracy, using 0.50 threshold\n",
    "threshold = 0.50\n",
    "pred_bools = [pl>threshold for pl in pred_labels]\n",
    "true_bools = [tl==1 for tl in true_labels]\n",
    "val_f1_accuracy = f1_score(true_bools,pred_bools,average='macro')*100\n",
    "val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "print('F1 Macro Validation Accuracy: ', val_f1_accuracy)\n",
    "print('Flat Validation Accuracy: ', val_flat_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tracked-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address\n",
      "[[153   0]\n",
      " [ 47   0]]\n",
      "bank_acct\n",
      "[[198   0]\n",
      " [  2   0]]\n",
      "credit_card\n",
      "[[197   0]\n",
      " [  3   0]]\n",
      "email\n",
      "[[183   0]\n",
      " [ 17   0]]\n",
      "govt_id\n",
      "[[181   0]\n",
      " [ 19   0]]\n",
      "name\n",
      "[[192   0]\n",
      " [  8   0]]\n",
      "password\n",
      "[[194   0]\n",
      " [  6   0]]\n",
      "phone_num\n",
      "[[196   0]\n",
      " [  4   0]]\n",
      "secret_keys\n",
      "[[200   0]\n",
      " [  0   0]]\n",
      "user\n",
      "[[160   0]\n",
      " [ 29  11]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for each label\n",
    "\n",
    "for label, cf in zip(label_names, multilabel_confusion_matrix(true_bools, pred_bools)):\n",
    "                     print(label)\n",
    "                     print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-structure",
   "metadata": {},
   "source": [
    "## Save model file\n",
    "\n",
    "If we're using data parallel save model as module, so you can use it either inside or outside of a multi-gpu environment later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sharing-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if n_gpu > 1:\n",
    "#    torch.save(model.module.state_dict(), \"path/to/your-model-name.pth\")\n",
    "#else:\n",
    "#    torch.save(model.state_dict(), \"path/to/your-model-name.pth\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-treaty",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using pretrained BERT models (`bert-base-cased` or `mini-bert`) from the huggingface repo and a custom traning for multi-label classification, we are able to successfully train a PII detector from our training dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
