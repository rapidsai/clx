{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithm (DGA) Detection\n",
    "\n",
    "## Authors\n",
    " - Gorkem Batmaz (NVIDIA) [gbatmaz@nvidia.com]\n",
    " - Bhargav Suryadevara (NVIDIA) [bsuryadevara@nvidia.com]\n",
    "\n",
    "## Development Notes\n",
    "* Developed using: RAPIDS v0.12.0 and CLX v0.12\n",
    "* Last tested using: RAPIDS v0.12.0 and CLX v0.12 on Jan 28, 2020\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Data Importing\n",
    "* Data Preprocessing\n",
    "* Training and Evaluation\n",
    "* Inference\n",
    "* Conclusion\n",
    "\n",
    "## Introduction\n",
    "[Domain Generation Algorithms](https://en.wikipedia.org/wiki/Domain_generation_algorithm) (DGAs) are used to generate domain names that can be used by the malware to communicate with the command and control servers. IP addresses and static domain names can be easily blocked, and a DGA provides an easy method to generate a large number of domain names and rotate through them to circumvent traditional block lists. We will use a type of recurrent neural network called the [Gated Recurrent Unit](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) (GRU) for this example. The [CLX](https://github.com/rapidsai/clx) and [RAPIDS](https://rapids.ai) libraries enable users train their models with up-to-date domain names representative of both benign and DGA generated strings. Using a CLX workflow, this capability could also be used in production. This notebook provides a view into the data science workflow to create a DGA detection implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clx/lib/python3.7/site-packages/treelite/gallery/__init__.py:7: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/clx/lib/python3.7/site-packages/treelite/gallery/sklearn/__init__.py:9: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "import time\n",
    "import cudf\n",
    "import torch\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from clx.analytics.detector_dataset import DetectorDataset\n",
    "from clx.analytics.dga_detector import DGADetector\n",
    "from cuml.preprocessing.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing\n",
    "Links used for examples of DGA generated domains and benign domains are below. Change these locations if you have a preferred alternative list.\n",
    "- DGA : http://osint.bambenekconsulting.com/feeds/dga-feed.txt\n",
    "- Benign : http://s3.amazonaws.com/alexa-static/top-1m.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_META_LIST = [\n",
    "    {\n",
    "        \"source\": \"DGA\",\n",
    "        \"url\": \"http://osint.bambenekconsulting.com/feeds/dga-feed.txt\",\n",
    "        \"compression\": None,\n",
    "        \"storage_path\": \"/input/dga_feed\",\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"Benign\",\n",
    "        \"url\": \"http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\",\n",
    "        \"compression\": \"zip\",\n",
    "        \"storage_path\": \"/input/top-1m\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(url_meta_list):\n",
    "    for entry in url_meta_list:\n",
    "        output_dir = entry['storage_path']\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "        os.makedirs(output_dir)\n",
    "        filepath = wget.download(entry['url'], out=output_dir)\n",
    "        unpack(entry['compression'], filepath, output_dir)\n",
    "        print('%s data is stored to location %s' %(entry['source'], output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, one of the files is compressed. We'll define a funciton that can decompress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(compression_type, filepath, output_dir):\n",
    "     if compression_type == 'zip':\n",
    "        with zipfile.ZipFile(filepath, 'r') as f:\n",
    "            f.extractall(output_dir)\n",
    "        os.remove(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the example domain name lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGA data is stored to location /input/dga_feed\n",
      "Benign data is stored to location /input/top-1m\n"
     ]
    }
   ],
   "source": [
    "download_files(URL_META_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to preprocess the downloaded data to get it ready for downstream modeling. We can do this using a [cuDF](https://github.com/rapidsai/cudf) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_data(url_meta_list):\n",
    "    dga_df = cudf.read_csv(url_meta_list[0]['storage_path'] + '/*', names=[\"domain\"], skiprows=14)\n",
    "    dga_df['type'] = 0\n",
    "    benign_df = cudf.read_csv(url_meta_list[1]['storage_path'] + '/*', names=[\"line_num\",\"domain\"])\n",
    "    benign_df = benign_df.drop('line_num')\n",
    "    benign_df['type'] = 1\n",
    "    input_df = cudf.concat([benign_df, dga_df], ignore_index=True)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function and load the data into a cuDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = load_input_data(URL_META_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train and Test Dataset\n",
    "We utilize the [`train_test_split` function](https://docs.rapids.ai/api/cuml/0.10/api.html#model-selection-and-data-splitting) from [cuML](https://github.com/rapidsai/cuml) and create a shuffled dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_train, domain_test, type_train, type_test = train_test_split(input_df, 'type', train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(domain_df, type_series):\n",
    "    df = cudf.DataFrame()\n",
    "    df['domain'] = domain_df['domain'].reset_index(drop=True)\n",
    "    df['type'] = type_series.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = create_df(domain_test, type_test)\n",
    "train_df = create_df(domain_train, type_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have only benign and DGA (malicious) categoriesm, the number of domain types need to be set to 2 (`N_DOMAIN_TYPE=2`). Vocabulary size(`CHAR_VOCAB`) is set to 128 ASCII characters. The values below set for `HIDDEN_SIZE`, `N_LAYERS` of the network, and the `LR` (Learning Rate) give an optimum balance for the network size and performance. They might need be set via experiments when working with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "N_LAYERS = 3\n",
    "CHAR_VOCAB = 128\n",
    "HIDDEN_SIZE = 100\n",
    "N_DOMAIN_TYPE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate DGA Detector\n",
    "Now that the data is ready, the datasets are created, and we've set the parameters for the model, we can use the DGADetector method built into CLX to create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DGADetector(lr=LR)\n",
    "dd.init_model(n_layers=N_LAYERS, char_vocab=CHAR_VOCAB, hidden_size=HIDDEN_SIZE, n_domain_type=N_DOMAIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Batches\n",
    "We need to partition the input dataframe into one or more smaller dataframes per the given batch size for training and testing of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "train_dataset = DetectorDataset(train_df, batch_size)\n",
    "test_dataset = DetectorDataset(test_df, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):\n",
    "    print(\"Verify if directory `%s` is already exists.\" % (dir_path))\n",
    "    if not os.path.exists(dir_path):\n",
    "        print(\"Directory `%s` does not exists.\" % (dir_path))\n",
    "        print(\"Creating directory `%s` to store trained models.\" % (dir_path))\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cache():\n",
    "    # release memory.\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(dd, train_dataset, test_dataset, epoch, model_dir):\n",
    "    print(\"Initiating model training\")\n",
    "    create_dir(model_dir)\n",
    "    max_accuracy = 0\n",
    "    prev_model_file_path = \"\"\n",
    "    for i in range(1, epoch + 1):\n",
    "        print(\"---------\")\n",
    "        print(\"Epoch: %s\" % (i))\n",
    "        print(\"---------\")\n",
    "        dd.train_model(train_dataset)\n",
    "        accuracy = dd.evaluate_model(test_dataset)\n",
    "        now = datetime.now()\n",
    "        output_filepath = (\n",
    "            model_dir\n",
    "            + \"/\"\n",
    "            + \"rnn_classifier_{}.pth\".format(now.strftime(\"%Y-%m-%d_%H_%M_%S\"))\n",
    "        )\n",
    "        if accuracy > max_accuracy:\n",
    "            dd.save_model(output_filepath)\n",
    "            max_accuracy = accuracy\n",
    "            if prev_model_file_path:\n",
    "                os.remove(prev_model_file_path)\n",
    "            prev_model_file_path = output_filepath\n",
    "    print(\"Model with highest accuracy (%s) is stored to location %s\" % (max_accuracy, prev_model_file_path))\n",
    "    return prev_model_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Using the function we created above, we now train and evaluate the model.\n",
    "*NOTE: You may see warnings when you run the training due to a [bug in PyTorch](https://github.com/pytorch/pytorch/issues/27972) which is being actively investigated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating model training\n",
      "Verify if directory `/trained_models` is already exists.\n",
      "---------\n",
      "Epoch: 1\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clx/lib/python3.7/site-packages/cudf/io/dlpack.py:74: UserWarning: WARNING: cuDF to_dlpack() produces column-major (Fortran order) output. If the output tensor needs to be row major, transpose the output of this function.\n",
      "  return libdlpack.to_dlpack(gdf_cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100000/822903 (12%)]\tLoss: 6531.22\n",
      "[200000/822903 (24%)]\tLoss: 4459.18\n",
      "[300000/822903 (36%)]\tLoss: 3394.05\n",
      "[400000/822903 (49%)]\tLoss: 2833.66\n",
      "[500000/822903 (61%)]\tLoss: 2526.76\n",
      "[600000/822903 (73%)]\tLoss: 2357.34\n",
      "[700000/822903 (85%)]\tLoss: 2236.31\n",
      "[800000/822903 (97%)]\tLoss: 2049.20\n",
      "Test set: Accuracy: 296977/352673 (0.8420746697365549)\n",
      "\n",
      "---------\n",
      "Epoch: 2\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 1916.30\n",
      "[200000/822903 (24%)]\tLoss: 1350.61\n",
      "[300000/822903 (36%)]\tLoss: 1177.15\n",
      "[400000/822903 (49%)]\tLoss: 1081.47\n",
      "[500000/822903 (61%)]\tLoss: 1051.81\n",
      "[600000/822903 (73%)]\tLoss: 1093.94\n",
      "[700000/822903 (85%)]\tLoss: 1133.28\n",
      "[800000/822903 (97%)]\tLoss: 1076.01\n",
      "Test set: Accuracy: 324903/352673 (0.9212585029191347)\n",
      "\n",
      "---------\n",
      "Epoch: 3\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 275.22\n",
      "[200000/822903 (24%)]\tLoss: 507.93\n",
      "[300000/822903 (36%)]\tLoss: 538.07\n",
      "[400000/822903 (49%)]\tLoss: 576.93\n",
      "[500000/822903 (61%)]\tLoss: 610.12\n",
      "[600000/822903 (73%)]\tLoss: 710.06\n",
      "[700000/822903 (85%)]\tLoss: 774.06\n",
      "[800000/822903 (97%)]\tLoss: 747.35\n",
      "Test set: Accuracy: 332940/352673 (0.9440473186209322)\n",
      "\n",
      "---------\n",
      "Epoch: 4\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 205.95\n",
      "[200000/822903 (24%)]\tLoss: 274.61\n",
      "[300000/822903 (36%)]\tLoss: 388.45\n",
      "[400000/822903 (49%)]\tLoss: 408.48\n",
      "[500000/822903 (61%)]\tLoss: 460.95\n",
      "[600000/822903 (73%)]\tLoss: 562.21\n",
      "[700000/822903 (85%)]\tLoss: 647.04\n",
      "[800000/822903 (97%)]\tLoss: 630.00\n",
      "Test set: Accuracy: 336141/352673 (0.9531237151695764)\n",
      "\n",
      "---------\n",
      "Epoch: 5\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 185.06\n",
      "[200000/822903 (24%)]\tLoss: 204.79\n",
      "[300000/822903 (36%)]\tLoss: 437.50\n",
      "[400000/822903 (49%)]\tLoss: 436.03\n",
      "[500000/822903 (61%)]\tLoss: 458.88\n",
      "[600000/822903 (73%)]\tLoss: 526.40\n",
      "[700000/822903 (85%)]\tLoss: 596.00\n",
      "[800000/822903 (97%)]\tLoss: 579.11\n",
      "Test set: Accuracy: 339462/352673 (0.962540370258001)\n",
      "\n",
      "---------\n",
      "Epoch: 6\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 178.96\n",
      "[200000/822903 (24%)]\tLoss: 180.35\n",
      "[300000/822903 (36%)]\tLoss: 207.92\n",
      "[400000/822903 (49%)]\tLoss: 268.50\n",
      "[500000/822903 (61%)]\tLoss: 308.16\n",
      "[600000/822903 (73%)]\tLoss: 460.86\n",
      "[700000/822903 (85%)]\tLoss: 525.48\n",
      "[800000/822903 (97%)]\tLoss: 514.59\n",
      "Test set: Accuracy: 341437/352673 (0.9681404587252214)\n",
      "\n",
      "---------\n",
      "Epoch: 7\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 168.57\n",
      "[200000/822903 (24%)]\tLoss: 165.98\n",
      "[300000/822903 (36%)]\tLoss: 184.44\n",
      "[400000/822903 (49%)]\tLoss: 265.49\n",
      "[500000/822903 (61%)]\tLoss: 342.65\n",
      "[600000/822903 (73%)]\tLoss: 403.21\n",
      "[700000/822903 (85%)]\tLoss: 476.06\n",
      "[800000/822903 (97%)]\tLoss: 466.78\n",
      "Test set: Accuracy: 343451/352673 (0.9738511312178704)\n",
      "\n",
      "---------\n",
      "Epoch: 8\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 168.18\n",
      "[200000/822903 (24%)]\tLoss: 154.01\n",
      "[300000/822903 (36%)]\tLoss: 165.55\n",
      "[400000/822903 (49%)]\tLoss: 204.81\n",
      "[500000/822903 (61%)]\tLoss: 255.79\n",
      "[600000/822903 (73%)]\tLoss: 359.05\n",
      "[700000/822903 (85%)]\tLoss: 416.83\n",
      "[800000/822903 (97%)]\tLoss: 410.27\n",
      "Test set: Accuracy: 344778/352673 (0.9776138235702762)\n",
      "\n",
      "---------\n",
      "Epoch: 9\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 163.03\n",
      "[200000/822903 (24%)]\tLoss: 142.08\n",
      "[300000/822903 (36%)]\tLoss: 150.24\n",
      "[400000/822903 (49%)]\tLoss: 196.22\n",
      "[500000/822903 (61%)]\tLoss: 238.68\n",
      "[600000/822903 (73%)]\tLoss: 349.08\n",
      "[700000/822903 (85%)]\tLoss: 403.60\n",
      "[800000/822903 (97%)]\tLoss: 396.65\n",
      "Test set: Accuracy: 345239/352673 (0.9789209834605995)\n",
      "\n",
      "---------\n",
      "Epoch: 10\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 151.71\n",
      "[200000/822903 (24%)]\tLoss: 132.66\n",
      "[300000/822903 (36%)]\tLoss: 139.08\n",
      "[400000/822903 (49%)]\tLoss: 162.51\n",
      "[500000/822903 (61%)]\tLoss: 215.81\n",
      "[600000/822903 (73%)]\tLoss: 317.51\n",
      "[700000/822903 (85%)]\tLoss: 373.32\n",
      "[800000/822903 (97%)]\tLoss: 368.76\n",
      "Test set: Accuracy: 345874/352673 (0.9807215182336045)\n",
      "\n",
      "---------\n",
      "Epoch: 11\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 138.97\n",
      "[200000/822903 (24%)]\tLoss: 119.78\n",
      "[300000/822903 (36%)]\tLoss: 126.10\n",
      "[400000/822903 (49%)]\tLoss: 145.44\n",
      "[500000/822903 (61%)]\tLoss: 197.61\n",
      "[600000/822903 (73%)]\tLoss: 266.33\n",
      "[700000/822903 (85%)]\tLoss: 337.40\n",
      "[800000/822903 (97%)]\tLoss: 334.48\n",
      "Test set: Accuracy: 346716/352673 (0.9831089989877309)\n",
      "\n",
      "---------\n",
      "Epoch: 12\n",
      "---------\n",
      "[100000/822903 (12%)]\tLoss: 135.32\n",
      "[200000/822903 (24%)]\tLoss: 114.86\n",
      "[300000/822903 (36%)]\tLoss: 119.06\n",
      "[400000/822903 (49%)]\tLoss: 136.45\n",
      "[500000/822903 (61%)]\tLoss: 180.20\n",
      "[600000/822903 (73%)]\tLoss: 247.72\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 30\n",
    "model_dir='/trained_models'\n",
    "model_filepath = train_and_eval(dd, train_dataset, test_dataset, epoch, model_dir)\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model generated above, we now score the test dataset against the model to determine if the domain is likely generated by a DGA or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DGADetector()\n",
    "dd.load_model(model_filepath)\n",
    "\n",
    "pred_results = []\n",
    "true_results = []\n",
    "for partition in test_dataset.partitioned_dfs:\n",
    "    pred_results.append(list(dd.predict(partition['domain']).values_host))\n",
    "    true_results.append(list(partition['type'].values_host))\n",
    "pred_results = np.concatenate(pred_results)\n",
    "true_results = np.concatenate(true_results)\n",
    "accuracy_score = accuracy_score(pred_results, true_results)\n",
    "print('Model accuracy: %s'%(accuracy_score))\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(true_results, pred_results)\n",
    "\n",
    "print('Average precision score: {0:0.3f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGA detector in CLX enables users to train their models for detection and also use existing models. This capability could also be used in conjunction with log parsing efforts if the logs contain domain names. DGA detection done with CLX and RAPIDS keeps data in GPU memory, removing unnecessary copy/converts and providing a 4X speed advantage over CPU only implementations. This is esepcially true with large batch sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
