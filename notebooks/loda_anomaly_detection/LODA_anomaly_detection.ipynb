{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection using Lightweight Online Detector of Anomalies (LODA) with CuPy and RAPIDS. \n",
    "\n",
    "### Authors:\n",
    "Tadesse ZeMicheal (NVIDIA)\n",
    "### Introduction\n",
    "\n",
    "Anomaly detection is an important problem that has been studied within wide areas and application domains. Several anomaly detection algorithms are generic while many are developed specifically to the domain of interest. In practice, several ensemble-based anomaly detection algorithms have been shown to have superior performance on many benchmark datasets, namely Isolation Forest, Lightweight Online Detector of Anomalies (LODA), and an ensemble of Gaussian mixture models ...etc.\n",
    "\n",
    "The Loda algorithm is one of the good performing generic anomaly detection algorithms. Loda detects anomalies in a dataset by computing the likelihood of data points using an ensemble of one-dimensional histograms. These histograms serve as density estimators by approximating the joint probability of the data using sparse random projections. For convenience, a negative log-likelihood (NLL) is used as scoring criteria, where a large NLL value indicates an instance to be anomalous. This is because anomalies are expected to be points with smaller likelihood under the fitted probability model (the histograms in this case).\n",
    "\n",
    "Loda is computationally fast and easily adopted for data streams. It is linear with respect to training sample $n$ and data dimension $d$. This makes it a good candidate to use as an anomaly detector model for many cybersecurity use cases, such as intrusion detection, behavioral analytics, sensor outages ...etc. Additionally, Loda can be easily adopted to an online-learning algorithm by implementing the histogram as an [online-histograms](https://jmlr.org/papers/volume11/ben-haim10a/ben-haim10a.pdf). \n",
    "\n",
    "In simple term Loda is constructed as a $k$ one-dimensional histograms ${h}^k_{i=1}$, where each histogram approximate probability density $p$ of input data projected into one-dimensional sparse projection vector $\\{w_i \\in R^d\\}^k_{i=1}$. The histogram is set as fixed-width and the number of bins is set as hyperparameter or computed heuristically. During training, the algorithm first constructs projection vectors $w_i$ by starting all-zero vector, selecting $k=\\sqrt{d}$ features at random, and replacing this with a random number generated from a standard normal distribution. Then a training example $X$ is projected to the real lines as $w^T_iX$, and a fixed-width histogram density estimator $p_i$ is estimated from the projected data. \n",
    "\n",
    "To compute the anomaly score of a query point $X_q$, Loda computes the average negative log likelihood (NLL) as follows: $$f(X_q) = \\frac{1}{k}\\sum_i^k -\\log p_i(w_i^TX_q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation \n",
    "\n",
    "Loda being a projection based algorithm makes it a good choice to use GPUs for the matrix operations involved. For both training and inference, the ensemble of random projection vectors $w^T_iX$ can be implemented efficiently using cupy and parallelize across GPUs for further acceleration.\n",
    "We implement the `Loda` class using `CuPy` in the `loda.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp \n",
    "import cudf, cuml \n",
    "import matplotlib.pylab as plt \n",
    "import cuml.metrics as mt\n",
    "import wget\n",
    "import requests;\n",
    "from os import path;\n",
    "%matplotlib inline \n",
    "\n",
    "from clx.analytics.loda import Loda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "To test the implementation of the algorithm we use anomaly dataset benchmark from [UCI dataset](https://archive.ics.uci.edu/ml/datasets.php). For this particular example we use the Statlog (shuttle) dataset. The dataset consistst of 9 features with approximately 80% belong to nominal class and remaining to anomaly class. In the following experiment we evaluate the Loda algorithm how good is in ranking the anomalies over the nominal class.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUTTLE_CSV = \"shuttle.csv\"\n",
    "DATA_BASE_URL = \"https://data.rapids.ai/cyber/clx/\"\n",
    "\n",
    "if not path.exists(SHUTTLE_CSV):\n",
    "    r = requests.get(DATA_BASE_URL + SHUTTLE_CSV)\n",
    "    open(SHUTTLE_CSV, 'wb').write(r.content)\n",
    "    \n",
    "df = cudf.read_csv(SHUTTLE_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nominal</td>\n",
       "      <td>49</td>\n",
       "      <td>-1</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nominal</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nominal</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-17</td>\n",
       "      <td>41</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nominal</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>46</td>\n",
       "      <td>80</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nominal</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  v1  v2  v3  v4  v5  v6  v7  v8  v9\n",
       "0  nominal  49  -1  77   0  50   5  28  28   0\n",
       "1  nominal  40   1  88   0  38   0  47  49   2\n",
       "2  nominal  37   0  77   0  -2 -17  41  80  40\n",
       "3  nominal  37   0  83   0   2  18  46  80  34\n",
       "4  nominal  55   0  83   0  56  25  27  26   0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset (12345, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the dataset {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = (df.iloc[:, 0] == \"anomaly\").astype('int'), df.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_score(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Compute average precision score using precision and recall computed from cuml. \n",
    "    \"\"\"\n",
    "    precision, recall, _ = mt.precision_recall_curve(y_true, y_score) #, pos_label=pos_label, sample_weight=sample_weight)\n",
    "    # return step function integral \n",
    "    return -cp.sum(cp.diff(recall) * cp.array(precision)[:-1])\n",
    "\n",
    "def metrics(y_true, y_score):\n",
    "    auc = mt.roc_auc_score(y_true=y_true, y_score=y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    # ap = mt.average_precision_score(y_true, y_score) # future implementation. \n",
    "    return [auc, ap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Scoring \n",
    "Training LODA object involves two steps, first projecting training sample into $|w|$ projections and fitting the projected data ($W^TX$) into $k$ histograms crossponding to each vector without any supervision. We set the parameter for the number of projections (`n_random_cuts`) and the number of bins (equil width bins) or optionally use heuristic to determine the number of bins.\n",
    "\n",
    "Once the histograms are fitted (Loda is trained), these  histogram serve as an ensemble of weak anomaly detectors by estimating the density of the random projected vectors. Combining these `n_random_cuts` histograms leads to a strong anomlay detector where the negative likelihood across the histograms serves as anomaly scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit loda with 500 random projections and number of bins are computed heuristically\n",
    "ld = Loda(n_random_cuts=500, n_bins=None) \n",
    "ld.fit(x) # train the detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate benchmark dataset\n",
    "We compute AUC and average precision score to evaluate the performance of the detector. We use  `cuml.metrics` to compute both AUC and average precision score (AP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00922115 0.00845244 0.01234676 ... 0.00923621 0.01334979 0.0090719 ]\n"
     ]
    }
   ],
   "source": [
    "score = ld.score(x) #generate nll scores\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC and Average precision [0.9901202917098999, array(0.7797753)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"AUC and Average precision {metrics(y, score)}\") # compute auc and pr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of anomalies \n",
    "\n",
    "To explain the cause of anomalies Loda utilize contributions of each feature across the histograms. Recognize that each histogram $H$ are made with sparse projections $w$, this method can be used to rank each features according to their contributions to the instance anomalousness. To measure the importance of a particular feature, the anomaly score difference is computed from all histograms when the feature is selected in the projection from all the histograms when the feature is not selected. This allows to measure how influential the feature is in isolating an instance as anomaly. \n",
    "\n",
    "Let $I_j$ denotes a vector score of histograms that use feature $j$, and let $\\bar{I}_j$ be the vector score of histograms that don't use feature $j$ and suppose the corresponding mean and variance of the scores are denoted as ($\\mu_j, s^2_j$)and $(\\bar{\\mu_j},\\bar{s^2_j})$ respectively.  Then anomaly explanation of a feature is given by one-tailed two-sample $t$ test with a test statistics given as follows. \n",
    "$$t_j: \\frac{\\mu_j - \\bar{\\mu}_j}{\\sqrt{\\frac{s^2_j}{I_j} + \\frac{\\bar{s}^2_j}{\\bar{I}_j}}}$$\n",
    "\n",
    "The purpose of this score function is to assert if the contribution of a feature $j$ is statistically significant on isolating the instance as anomalies. Larger t-score indicates the more influence the feature has on identifying the instance as anomaly. We rank each feature based on their $t_{score}$ to output the most influential features for the anomalousness of the instance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort top instance based on their anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_score = cudf.DataFrame() \n",
    "ordered_score['label'] = df.label\n",
    "ordered_score['score'] = score\n",
    "ordered_score = ordered_score.sort_values( by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.044730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3066</th>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.041863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.039846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12013</th>\n",
       "      <td>anomaly</td>\n",
       "      <td>0.039467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label     score\n",
       "780    anomaly  0.050395\n",
       "3680   anomaly  0.044730\n",
       "3066   anomaly  0.041863\n",
       "10874  anomaly  0.039846\n",
       "12013  anomaly  0.039467"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_score.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate an explanation we will take examples of top ranked anomalies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_anomaly = df.iloc[780, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1      79\n",
       "v2       0\n",
       "v3      83\n",
       "v4       0\n",
       "v5      98\n",
       "v6    8098\n",
       "v7       4\n",
       "v8     -14\n",
       "v9     -18\n",
       "Name: 780, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check explanation from Loda\n",
    "feature_explanation = ld.explain(selected_anomaly.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance scores: [0.44455455 0.         0.33849765 0.28921792 0.59852637 1.\n",
      " 0.27721291 0.6492718  0.48685219]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Feature importance scores: {feature_explanation.ravel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-score values are normalized between 0 and 1, the plot below shows comparison of each features influence for the selected example. Clearly, feature $V6$ has higher influence followed by feature $V5$ and $V8$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Features')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the contribution of each features. \n",
    "plt.bar(df.columns[1:], feature_explanation.ravel().get())\n",
    "plt.ylabel('T-score')\n",
    "plt.xlabel('Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the explanation, we can check how each feature of the selected example is deviated from the median feature value. We use median abosolute deviation (MAD) to measure how far is each feature from its normalized MAD score. MAD = $median(|x - median(x)|)/scale$. Not that, this is not necessarly a direct match to the feature explanation produced by Loda but at least it gives how far each feature are from the median statistics of the dataset population. As a result we expect at least the top deviated feature to match against Loda feature explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad(x, axis=1):\n",
    "    # return median absolute deviation \n",
    "    standard_scale = 0.67448975\n",
    "    return cp.median(cp.abs(x - cp.median(x, axis=axis)), axis=axis)/standard_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_absolute_deviation_features = mad(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deviation of each features from mad [ 6.86217845e+01  0.00000000e+00  7.70695911e+01  0.00000000e+00\n",
      "  8.61391822e+01  8.09058699e+03 -3.41301109e+00 -2.43782155e+01\n",
      " -2.09652044e+01]\n"
     ]
    }
   ],
   "source": [
    "mad_distance = selected_anomaly.values - median_absolute_deviation_features\n",
    "print(f'Deviation of each features from mad {mad_distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook we show a GPU implementation of Loda algorithm for anomaly detection and explanation. We evaluate the implementation on anomaly benchmark dataset to identify anomalies and explain the features that cause for its identification using RAPDIS. In the future work we will show use of Loda for different cybersecurity usecases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [Loda: Lightweight on-line detector of anomalies](https://link.springer.com/article/10.1007/s10994-015-5521-0)\n",
    "- [PyOD: A Python Toolbox for Scalable Outlier Detection](https://www.jmlr.org/papers/volume20/19-011/19-011.pdf)\n",
    "- [Anomaly Detection in the Presence of Missing Values](https://arxiv.org/pdf/1809.01605.pdf)\n",
    "- https://archive.ics.uci.edu/ml/datasets/Statlog+%28Shuttle%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
