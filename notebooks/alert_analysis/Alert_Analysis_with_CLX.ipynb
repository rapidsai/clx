{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Alert Notables from Splunk using CLX and RAPIDS\n",
    "\n",
    "## Authors\n",
    " - Rachel Allen, PhD (NVIDIA)\n",
    " - Bianca Rhodes (NVIDIA)\n",
    "\n",
    "## Development Notes\n",
    "* Developed using: RAPIDS v0.10.0 and CLX v0.11\n",
    "* Last tested using: RAPIDS v0.12.0 and CLX v0.12 on Feb 26, 2019\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Data Ingest\n",
    "* Historical Trends\n",
    "* Rolling Z-Scores for Anomaly Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "To keep pace with cyber adversaries, organizations are constantly evolving in their approaches to information monitoring. With the addition of every new alert generated by ML models, heuristics, or sensors comes an additional data feed in need of triage and analysis. SOCs are frequently overwhelmed by the volume of alerts and unable to analyze a large portion of their data, resulting in potentially missed malicious activity. By leveraging the data processing and analytic capabilities of [RAPIDS](https://rapids.ai), a suite of open-source software libraries that allow for end-to-end data science pipelines in GPU memory, we demonstrate how it's possible to explore, analyze, and prioritize massive amounts of cyber data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "# traditional viz\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook, curdoc\n",
    "from bokeh.themes import built_in_themes\n",
    "from bokeh.layouts import column, gridplot\n",
    "from bokeh.models import Span\n",
    "\n",
    "# rapids io/df\n",
    "import cudf\n",
    "import dask_cudf\n",
    "\n",
    "# rapids clx \n",
    "from clx.parsers.splunk_notable_parser import SplunkNotableParser\n",
    "from clx import analytics\n",
    "from clx.analytics.stats import rzscore\n",
    "\n",
    "# rapids viz\n",
    "from cuxfilter import charts, layouts, themes, DataFrame\n",
    "from cuxfilter.charts import datashader, bokeh\n",
    "import panel as pn\n",
    "\n",
    "import requests\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are these libraries?\n",
    "\n",
    "CLX (Cyber Log Accelerators) provides a simple API for security analysts, data scientists, and engineers to quickly get started applying RAPIDS to real-world cyber use cases. CLX uses the GPU dataframe ([cuDF](https://github.com/rapidsai/cudf)) and other RAPIDS packages to execute cybersecurity and information security workflows. The following packages are available:\n",
    "\n",
    "* mlstats - Machine learning and statistics functionality\n",
    "* ip - IPv4 data translation and parsing\n",
    "* parsers - Cyber log event parsing\n",
    "* io - Input and output features for a workflow\n",
    "* workflow - Workflow which receives input data and produces analytical output data\n",
    "* osi - Open source integration (VirusTotal, FarsightDB)\n",
    "* dns - TLD, SLD extraction\n",
    "\n",
    "\n",
    "## When to use [CLX](https://github.com/rapidsai/clx)\n",
    "\n",
    "Use CLX to build your cyber data analytics workflows for a GPU-accelerated environmetn using RAPIDS. CLX contains common cyber and cyber ML functionality, such as log parsing for specific data sources, cyber data type parsing (e.g., IPv4), and DGA detection. CLX also provides the ability to integrate this functionality into a CLX workflow, which simplifies execution of the series of parsing and ML functions needed for end-to-end use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Alert Analysis on GPU with CLX and RAPIDS\n",
    "\n",
    "## Data Ingest with cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"./splunk_faker_raw4\"):\n",
    "    r = requests.get('https://data.rapids.ai/cyber/clx/splunk_faker_raw4')\n",
    "    open('./splunk_faker_raw4', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cuDF\n",
    "gdf = cudf.read_csv('./splunk_faker_raw4')\n",
    "gdf.columns = ['raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pandas\n",
    "pdf = pd.read_csv('./splunk_faker_raw4')\n",
    "pdf.columns = ['raw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noteable Event Parsing from Splunk\n",
    "CLX provides parsers to parse common log types including notable events indexed in Splunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clx.parsers.splunk_notable_parser import SplunkNotableParser\n",
    "snp = SplunkNotableParser()\n",
    "parsed_gdf = cudf.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# clx\n",
    "parsed_gdf = snp.parse(gdf, 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_gdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pandas\n",
    "\n",
    "pdf['time'] = pdf['raw'].str.extract('(^[0-9]+\\.?[0-9]*),')\n",
    "pdf['search_name'] = pdf['raw'].str.extract('search_name=\\\"([0-9A-Za-z\\s\\-\\(\\)]+)')\n",
    "pdf['urgency'] = pdf['raw'].str.extract('urgency=\\\"([A-Za-z]+)')\n",
    "pdf['user'] = pdf['raw'].str.extract('user=\\\"([A-Za-z0-9]+)')\n",
    "pdf['owner'] = pdf['raw'].str.extract('owner=\\\"([\\w@\\.]+)')\n",
    "pdf['security_domain'] = pdf['raw'].str.extract('security_domain=\\\"([A-Za-z]+)')\n",
    "pdf['severity'] = pdf['raw'].str.extract('severity=\\\"([A-Za-z]+)')\n",
    "pdf['src_ip'] = pdf['raw'].str.extract('src_ip=\\\"([\\w\\.\\-]+)')\n",
    "pdf['src_mac'] = pdf['raw'].str.extract('smac=([\\w\\:]+)')\n",
    "pdf['src_port'] = pdf['raw'].str.extract('src_port=\\\"(\\d+)')\n",
    "pdf['dest_ip'] = pdf['raw'].str.extract('dest_ip=\\\"([\\w\\.\\-]+)')\n",
    "pdf['dest_mac'] = pdf['raw'].str.extract('dmac=([\\w\\:]+)')\n",
    "pdf['dest_port'] = pdf['raw'].str.extract('dest_port=\\\"(\\d+)')\n",
    "pdf['dest_priority'] = pdf['raw'].str.extract('dest_priority=\"([A-Za-z]+)')\n",
    "pdf['device_name'] = pdf['raw'].str.extract('Device Name:\\s([0-9A-Za-z\\_\\-]+)')\n",
    "pdf['event_name'] = pdf['raw'].str.extract('Event Name:\\s([A-Za-z\\_]+)')\n",
    "pdf['event_type'] = pdf['raw'].str.extract('Event Type:\\s([A-Za-z]+)')\n",
    "pdf['ip_address'] = pdf['raw'].str.extract('IP Address:\\s\\(([0-9\\.]+)')\n",
    "pdf['message_ip'] = pdf['raw'].str.extract('message.ip=\"([\\w\\.]+)')\n",
    "pdf['message_hostname'] = pdf['raw'].str.extract('message.hostname=\"([\\w\\.]+)')\n",
    "pdf['message_username'] = pdf['raw'].str.extract('message.user_name=\"([\\w\\.\\@]+)')\n",
    "pdf['message_description'] = pdf['raw'].str.extract('message\\.description=\"([\\w\\.\\s]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RAPIDS for fast ETL and `groupby` operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 2 functions - time rounded to day, time rounded to hour\n",
    "def round2day(epoch_time):\n",
    "    return int(epoch_time/86400)*86400\n",
    "\n",
    "def round2hour(epoch_time):\n",
    "    return int(epoch_time/3600)*3600  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cuDF\n",
    "\n",
    "parsed_gdf['time'] = parsed_gdf['time'].astype(int)\n",
    "\n",
    "parsed_gdf['day'] = parsed_gdf.time.applymap(round2day)\n",
    "parsed_gdf['hour'] = parsed_gdf.time.applymap(round2hour)\n",
    "\n",
    "# aggregated by hour\n",
    "hour_rule_gdf= parsed_gdf[['search_name','hour','time']].groupby(['search_name', 'hour']).count().reset_index()\n",
    "hour_rule_gdf.columns = ['rule', 'hour', 'count']\n",
    "\n",
    "# aggregated by day\n",
    "day_rule_gdf= parsed_gdf[['search_name','day','time']].groupby(['search_name', 'day']).count().reset_index()\n",
    "day_rule_gdf.columns = ['rule', 'day', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pandas\n",
    "\n",
    "pdf['time'] = pdf['time'].astype(int)\n",
    "\n",
    "pdf['day'] = pdf.time.apply(round2day)\n",
    "pdf['hour'] = pdf.time.apply(round2hour)\n",
    "\n",
    "# aggregated by hour\n",
    "hour_rule_pdf= pdf[['search_name','hour','time']].groupby(['search_name', 'hour']).count().reset_index()\n",
    "hour_rule_pdf.columns = ['rule', 'hour', 'count']\n",
    "\n",
    "# aggregated by day\n",
    "day_rule_pdf= pdf[['search_name','day','time']].groupby(['search_name', 'day']).count().reset_index()\n",
    "day_rule_pdf.columns = ['rule', 'day', 'count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap Visualization\n",
    "\n",
    "Create a heatmap of normalized alert counts by day using cuXfilter or seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize counts and convert strings\n",
    "\n",
    "def normalize_gdf(input_gdf, time_gdf):\n",
    "    \"\"\" normalize event counts in DataFrame by rule\n",
    "        :input_gdf: cudf DataFrame with 'rule' column\n",
    "        :time_gdf: cudf with a column for times at each interval, so there is an entry for every day or hour even if it's 0\n",
    "    \"\"\"\n",
    "\n",
    "    rules_gdf = input_gdf['rule'].unique()\n",
    "    for num in range(len(rules_gdf)):\n",
    "        rule = rules_gdf[num]\n",
    "        temp_df = cudf.DataFrame()\n",
    "        temp_df = input_gdf[input_gdf['rule'] == rule]\n",
    "        temp_df = time_gdf.merge(temp_df, how='left')\n",
    "        temp_df['count'] = temp_df['count'].fillna(0)\n",
    "        temp_df['normalized'] = (temp_df['count'] - temp_df['count'].mean())/ temp_df['count'].std()\n",
    "        temp_df['rule_num'] = num\n",
    "        temp_df['rule'] = rule\n",
    "        if num == 0:\n",
    "            output_df = temp_df\n",
    "        else:\n",
    "            output_df = cudf.concat([output_df, temp_df])\n",
    "    return output_df\n",
    "\n",
    "# create one dataframe for alerts grouped by hour, one for alerts grouped by day\n",
    "\n",
    "hours_gdf = cudf.DataFrame()\n",
    "hours_gdf['hour'] = list(range(hour_rule_gdf['hour'].min(), hour_rule_gdf['hour'].max(), 3600))\n",
    "\n",
    "hours_viz_df = normalize_gdf(hour_rule_gdf, hours_gdf)\n",
    "hours_viz_df['count'] = hours_viz_df['count'].astype(float)\n",
    "hours_viz_df['rule_num'] = hours_viz_df['rule_num'].astype(float)\n",
    "hours_viz_df['hour'] = hours_viz_df['hour'].astype(float)\n",
    "\n",
    "day_gdf = cudf.DataFrame()\n",
    "day_gdf['day'] = list(range(day_rule_gdf['day'].min(), day_rule_gdf['day'].max(), 86400))\n",
    "\n",
    "days_viz_df = normalize_gdf(day_rule_gdf, day_gdf)\n",
    "days_viz_df['count'] = days_viz_df['count'].astype(float)\n",
    "days_viz_df['rule_num'] = days_viz_df['rule_num'].astype(float)\n",
    "days_viz_df['day'] = days_viz_df['day'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cuXfilter\n",
    "\n",
    "[cuXfilter](https://github.com/rapidsai/cuxfilter) ( ku-cross-filter ) is a RAPIDS framework to connect web visualizations to GPU accelerated crossfiltering. Inspired by the javascript version of the original, it enables interactive and super fast multi-dimensional filtering of 100 million+ row tabular datasets via cuDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for in notebook viz\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cux_df = DataFrame.from_dataframe(hours_viz_df)\n",
    "chart1 = charts.datashader.heatmap(x='hour', y='rule_num', aggregate_col='normalized', point_size = 30, title = 'Alerts Per Hour By Type')\n",
    "d = cux_df.dashboard([chart1], layout=layouts.single_feature)\n",
    "chart1.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cux_df = DataFrame.from_dataframe(days_viz_df)\n",
    "chart2 = charts.datashader.heatmap(x='day', y='rule_num', aggregate_col='normalized', point_size = 30, title = 'Alerts Per Day By Type')\n",
    "d = cux_df.dashboard([chart2], layout=layouts.single_feature)\n",
    "chart2.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matplotlib visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_rule_pdf['day'] = pd.to_datetime(day_rule_pdf['day'], unit = 's').dt.strftime('%Y-%m-%d')\n",
    "day_rule_pdf = day_rule_pdf.pivot(index='day', columns='rule', values='count').fillna(0)\n",
    "normalized_df=(day_rule_pdf-day_rule_pdf.mean())/day_rule_pdf.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Z-Score with CLX `stats` Module\n",
    "Detect large changes in the volume of specific alerts over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clx.analytics.stats import rzscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot the alert data so each rule is a column\n",
    "\n",
    "def pivot_table(gdf, index_col, piv_col, v_col):\n",
    "    \"\"\" Create a spreadsheet-style pivot table as a DataFrame\n",
    "    \n",
    "        :gdf: cudf DataFrame\n",
    "        :ind ex_col: keys to group by on the pivot table index\n",
    "        :piv_col: keys to group by on the pivot table column\n",
    "        :v_col: column to aggregrate\n",
    "    \"\"\"\n",
    "    index_list = gdf[index_col].unique()\n",
    "    piv_gdf = cudf.DataFrame()\n",
    "    piv_gdf[index_col] = index_list\n",
    "    groups_gdf = gdf[piv_col].unique()\n",
    "    for i in range(len(groups_gdf)):\n",
    "        group = groups_gdf[i]\n",
    "        temp_df = gdf[gdf[piv_col] == group]\n",
    "        temp_df = temp_df[[index_col, v_col]]\n",
    "        temp_df.columns = [index_col, group]\n",
    "        piv_gdf = piv_gdf.merge(temp_df, on=[index_col], how='left')\n",
    "    piv_gdf = piv_gdf.set_index(index_col)\n",
    "    return piv_gdf.sort_index()\n",
    "\n",
    "alerts_per_day_piv = pivot_table(day_rule_gdf, 'day', 'rule', 'count').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new df with rolling zscores\n",
    "\n",
    "r_zscores = cudf.DataFrame()\n",
    "for rule in alerts_per_day_piv.columns:\n",
    "    x = alerts_per_day_piv[rule]\n",
    "    r_zscores[rule] = rzscore(x, 7) #7 day window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line charts highlighting anomalous rule frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cuXfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Access - Brute Force Access Behavior Detected - Rule'\n",
    "temp_df = cudf.DataFrame()\n",
    "temp_df['rule'] = alerts_per_day_piv[column_name]\n",
    "temp_df['day'] = temp_df.index\n",
    "\n",
    "        \n",
    "cux_df = DataFrame.from_dataframe(temp_df)\n",
    "chart3 = datashader.line('day', 'rule')\n",
    "\n",
    "d = cux_df.dashboard([chart3])\n",
    "\n",
    "chart3.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bokeh charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Dark2_5 as palette\n",
    "\n",
    "def anomaly_plot_bokeh(ts_df, zc_df):\n",
    "    \"\"\" Makes on chart for every column, adds red lines for timepoints that pass zscore threshold\n",
    "        :ts_df: timeseries DataFrame\n",
    "        :zc_df: rolling z score DataFrame\n",
    "    \"\"\"\n",
    "    colors = itertools.cycle(palette)     # create a color iterator\n",
    "    for column, color in zip(ts_df.columns.tolist(), colors):\n",
    "        p = figure(x_axis_type=\"datetime\", plot_width=1000, plot_height=100, title=column)\n",
    "        p.line(ts_df.index, ts_df[column], color=color, line_width = 2 )\n",
    "        p.title.text_font_size = '8pt'\n",
    "        p.xaxis.major_label_text_font_size = \"8pt\"\n",
    "        p.yaxis.major_label_text_font_size = \"8pt\"\n",
    "        p.xgrid.visible = False\n",
    "        xcoords = zc_df[column].loc[(zc_df[column]).abs() > 2.2].index.values\n",
    "        for xc in xcoords:\n",
    "            vert_line = Span(location=xc, dimension='height', line_color='red', line_width=3, line_alpha=0.5)\n",
    "            p.add_layout(vert_line)\n",
    "        show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving back to pandas for bokeh\n",
    "alerts_piv_pdf = alerts_per_day_piv.to_pandas()\n",
    "alerts_piv_pdf.index = pd.to_datetime(alerts_piv_pdf.index, unit = 's')\n",
    "r_zscores_pdf = r_zscores.to_pandas()\n",
    "r_zscores_pdf.index = pd.to_datetime(r_zscores_pdf.index, unit = 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart first 10 rules with anamoly highlights\n",
    "output_notebook()\n",
    "anomaly_plot_bokeh(alerts_piv_pdf.iloc[:,range(10)], r_zscores_pdf.iloc[:,range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The ability to visualize trends and highlight abnormalities in splunk alerts empowers cyber analysts to get more from the data they are already collecting. Instead of relying on individuals to keep a mental note of alert trends and co-occurences, CLX can keep up with the high velocity of data and highlight potentially overlooked alerts in need of analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
