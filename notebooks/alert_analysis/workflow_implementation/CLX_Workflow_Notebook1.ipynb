{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLX Workflow Overview (GTC DC 2019)\n",
    "*Notebook 1 of 3*\n",
    "\n",
    "\n",
    "## Author\n",
    " - Bianca Rhodes (NVIDIA) [brhodes@nvidia.com]\n",
    "\n",
    "## Development Notes\n",
    "* Developed using: RAPIDS v0.11.0 and CLX v0.11.0\n",
    "* Last tested using: RAPIDS v0.11.0 and CLX v0.11.0 on Nov 5, 2019\n",
    "\n",
    "\n",
    "## Introduction to the CLX Workflow\n",
    "\n",
    "This notebook demonstrates the concept of a [CLX](https://github.com/rapidsai/clx) workflow. A CLX workflow performs analytical operations on a GPU dataframe. It also manages I/O components, allowing it to receive input data from a file or Kafka in the format of a gpu dataframe and output data in that format as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Visualization](./image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a workflow that performs some operations on a GPU dataframe. For this we must subclass the CLX Workflow class, which handles the initiation of IO components and workflow processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clx.workflow.workflow import Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestWorkflowImpl(Workflow):\n",
    "    \n",
    "    # Define your data processing in a function called \"workflow\"\n",
    "    def workflow(self, dataframe):\n",
    "        dataframe[\"enriched\"] = dataframe[\"raw\"].str.len()\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prepare your input file if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw\n",
      "hello gtcdc"
     ]
    }
   ],
   "source": [
    "!cat $curr_path/input.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next indicate your source and destination for your workflow input and output. This can be a file or Kafka. For this test example, let's use a file. The underlying code uses cudf IO to read from a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = {\n",
    "    \"type\": \"fs\",\n",
    "    \"input_format\": \"csv\",\n",
    "    \"input_path\": curr_path + \"/input.csv\",\n",
    "    \"schema\": [\"raw\"],\n",
    "    \"delimiter\": \",\",\n",
    "    \"usecols\": [\"raw\"],\n",
    "    \"dtype\": [\"str\"],\n",
    "    \"header\": 0,\n",
    "}\n",
    "destination = {\n",
    "    \"type\": \"fs\",\n",
    "    \"output_format\": \"csv\",\n",
    "    \"output_path\": curr_path + \"/output.csv\",\n",
    "    \"index\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Instantiate your new workflow and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -e $curr_path/output.csv ] && rm $curr_path/output.csv\n",
    "workflow = TestWorkflowImpl(name=\"my-test-workflow\", source=source, destination=destination)\n",
    "workflow.run_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Inspect your output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"raw\",\"enriched\"\n",
      "\"hello gtcdc\",11\n"
     ]
    }
   ],
   "source": [
    "!cat $curr_path/output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( Talk about how this can easily be deployed to production )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLX Log Event Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLX Parsers use regex to extract meaningful key/value pairs from raw log event data. To implement your own CLX Event Parser you can subclass the EventParser class and indicate the regex values used to parse this particular type of event as well as the pre and prost processing methods as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Visualization](./image7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clx.parsers.event_parser import EventParser\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = cudf.DataFrame()\n",
    "test_input[\"raw\"] = [\"username=gtcdc host=1.2.3.4    \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the regex for the event log.  \n",
    "  \n",
    "    Here we specify that the key value `username` will be found in the log event as `username=([a-z\\.\\-0-9$]+)`. The value captured within the parentheses or group will be extracted as the value for the given key.  \n",
    "    \n",
    "    It is also an option to define this regex easily within a yaml file and import via a yaml file reader. Within the CLX Windows Event Log parser, we do so here:\n",
    "    https://github.com/rapidsai/clx/blob/branch-0.11/clx/parsers/resources/windows_event_regex.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_regex = {\n",
    "   \"username\": \"username=([a-z\\.\\-0-9$]+)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create your event parser. The event parser must contain a method named `parse`. This method will handle all functionality for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEventParser(EventParser):\n",
    "    def parse(self, dataframe, raw_column):\n",
    "        # First we can pre-process the data. Let's strip trailing space\n",
    "        dataframe[\"processed\"] = dataframe[\"raw\"].str.rstrip(\" \")\n",
    "        # Call parent class parse_raw_event method\n",
    "        parsed_dataframe = self.parse_raw_event(\n",
    "            dataframe, \"processed\", event_regex\n",
    "        )\n",
    "        return parsed_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gtcdc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  username\n",
       "0    gtcdc"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = TestEventParser(columns=[\"username\"], event_name=\"mylogevent\")\n",
    "parser.parse(test_input, \"raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating the parser and CLX workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to perform analytics on pre-parsed data. In this example, we'll show how to integrate the custom log parser within CLX and integrate it \n",
    "above into a CLX workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create input data file. This is a sample log that we will parse. Our goal is to extract username value `gtcdc` for our analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"raw\\n    username=gtcdc host=1.2.3.4    \" > input2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Establish source and destination parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = {\n",
    "    \"type\": \"fs\",\n",
    "    \"input_format\": \"csv\",\n",
    "    \"input_path\": curr_path + \"/input2.csv\",\n",
    "    \"schema\": [\"raw\"],\n",
    "    \"delimiter\": \",\",\n",
    "    \"required_cols\": [\"raw\"],\n",
    "    \"dtype\": [\"str\"],\n",
    "    \"header\": 0,\n",
    "}\n",
    "destination = {\n",
    "    \"type\": \"fs\",\n",
    "    \"output_format\": \"csv\",\n",
    "    \"output_path\": curr_path + \"/output2.csv\",\n",
    "    \"index\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create the custom workflow. This workflow first parses the data and then counts the characters in username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestWorkflowImpl(Workflow):\n",
    "    parser = TestEventParser(columns=[\"username\"], event_name=\"mylogevent\")\n",
    "    \n",
    "    # Define your data processing in a function called \"workflow\"\n",
    "    def workflow(self, dataframe):\n",
    "        output_dataframe = cudf.DataFrame()\n",
    "        parsed_dataframe = self.parser.parse(test_input, \"raw\")\n",
    "        output_dataframe[\"username\"] = parsed_dataframe[\"username\"]\n",
    "        output_dataframe[\"count\"] = parsed_dataframe[\"username\"].str.len()\n",
    "        return output_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -e $curr_path/output2.csv ] && rm $curr_path/output2.csv\n",
    "workflow = TestWorkflowImpl(name=\"my-test-workflow\", source=source, destination=destination)\n",
    "workflow.run_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Display the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $curr_path/output2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
